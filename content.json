{"meta":{"title":"AIR","subtitle":"A research team studying artificial intelligence","description":"AIR","author":"AIR@USC","url":"http://uscair.club","root":"/"},"pages":[{"title":"关于","date":"2020-02-06T08:22:15.191Z","updated":"2020-02-06T08:22:15.191Z","comments":true,"path":"about/index.html","permalink":"http://uscair.club/about/index.html","excerpt":"","text":"AIR@USCContact Address: the University of South China - 28 Changsheng West Road- Hunan, China E-mail: airusc@foxmail.com QQ：3458038461 Blog: https://uscair.club Introduction:A professional team studying artificial intelligence and robotics Instructor:Dr. Mao Yu Honor One First Prize &amp; Two Third Prizes, [The 15th Hunan University Student Computer Program Design Competition] Aug. 2019 ![The 15th Hunan University Student Computer Program Design Competition](https://s2.ax1x.com/2020/02/01/1Gy3xH.jpg) ![![1GyRoV.jpg](https://s2.ax1x.com/2020/02/01/1GyRoV.jpg)](https://s2.ax1x.com/2020/02/01/1GyRoV.jpg) One Second Prize &amp; Three Third Prizes,[“Soft Silver Robot Cup” China robot skill competition] Dec. 2018 ![1GgUd1.jpg](https://s2.ax1x.com/2020/02/01/1GgUd1.jpg) EssayGrade 16 Qiu Zhongxi published EI paper as first author ![1GcK3D.png](https://s2.ax1x.com/2020/02/01/1GcK3D.png)"},{"title":"所有分类","date":"2020-02-06T08:16:28.355Z","updated":"2020-02-06T08:16:28.355Z","comments":true,"path":"categories/index.html","permalink":"http://uscair.club/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-02-06T08:17:31.100Z","updated":"2020-02-06T08:17:31.100Z","comments":true,"path":"tags/index.html","permalink":"http://uscair.club/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数字图像处理——拉普拉斯算子【像素级别处理】（python）","slug":"数字图像处理——拉普拉斯算子【像素级别处理】（python）","date":"2020-02-06T12:34:46.135Z","updated":"2020-02-06T12:34:46.135Z","comments":true,"path":"2020/02/06/数字图像处理——拉普拉斯算子【像素级别处理】（python）/","link":"","permalink":"http://uscair.club/2020/02/06/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90%E3%80%90%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%88%AB%E5%A4%84%E7%90%86%E3%80%91%EF%BC%88python%EF%BC%89/","excerpt":"","text":"数字图像处理——拉普拉斯算子【像素级别处理】（python）简介：==拉普拉斯算子是一种微分算子常在图像处理中强调灰度值的突变，不强调灰度变换缓慢的地方，得到的图层与原图像叠加在一起可以得到锐化的效果== 一个二维图像的拉普拉斯算子可以定义为\\nabla^{2}f=\\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2} 所以:在X方向上存在\\frac{\\partial^2 f}{\\partial x}=f(x+1,y)+f(x-1,y)-2f(x,y) 在Y方向上存在\\frac{\\partial^2 f}{\\partial y}=f(x,y+1)+f(x,y-1)-2f(x,y) 可得： \\nabla^2f=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)扩展至对角线： \\nabla^2f=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)+f(x-1,y-1)+f(x-1,y+1)+f(x+1,y-1)+f(x+1,y+1)-8f(x,y)代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import cv2import numpy as npimport matplotlib.pyplot as pltimg = cv2.imread('Fig0338.tif') # 测试图片H = img.shape[0]W = img.shape[1]pixa = np.zeros((H, W), np.int32)mImgae = np.zeros((H, W, 3), np.uint8) # 标定(scale)前的滤波图像smImga = np.zeros((H, W, 3), np.uint8) # 标定(scale)后的滤波图像pixb = np.zeros((H, W), np.int32)mImgbe = np.zeros((H, W, 3), np.uint8) # 标定前的滤波图像smImgb = np.zeros((H, W, 3), np.uint8) # 标定后的滤波图像imga = np.zeros((H, W, 3), np.uint8) # xy方向模板滤波后图像imgb = np.zeros((H, W, 3), np.uint8) # 加上对角方向模板滤波后图像# a用到的算子是 b用到的算子是# 0 1 0 1 1 1# 1 -4 1 1 -8 1# 0 1 0 1 1 1# 先绘制标定滤波图像# 标定指的是最小值设置为0，最大值设置为255的进行归一化的结果for i in range(1, H - 1): for j in range(1, W - 1): pixa[i, j] = int(img[i - 1, j, 0]) + img[i + 1, j, 0] + img[i, j - 1, 0] + img[i, j + 1, 0] - 4 * int( img[i, j, 0]) pixb[i, j] = int(img[i - 1, j - 1, 0]) + img[i - 1, j, 0] + img[i - 1, j + 1, 0] + img[i, j - 1, 0] + img[ i, j + 1, 0] + img[i + 1, j - 1, 0] + img[i + 1, j, 0] + img[i + 1, j + 1, 0] - 8 * int(img[i, j, 0])maxa = 0maxb = 0mina = 255minb = 255for i in range(H): for j in range(W): # 求出像素最大值和最小值，以利于scale if pixa[i, j] &gt; maxa: maxa = pixa[i, j] if pixa[i, j] &lt; mina: mina = pixa[i, j] if pixb[i, j] &gt; maxb: maxb = pixb[i, j] if pixb[i, j] &lt; minb: minb = pixb[i, j] if pixa[i, j] &lt; 0: mImgae[i, j] = [0, 0, 0] else: mImgae[i, j, 0] = pixa[i, j] mImgae[i, j, 1] = pixa[i, j] mImgae[i, j, 2] = pixa[i, j] if pixb[i, j] &lt; 0: mImgbe[i, j] = [0, 0, 0] else: mImgbe[i, j, 0] = pixb[i, j] mImgbe[i, j, 1] = pixb[i, j] mImgbe[i, j, 2] = pixb[i, j]ka = 0kb = 0if maxa &gt; mina: ka = 255 / (maxa - mina)if maxb &gt; minb: kb = 255 / (maxb - minb)# scale处理for i in range(H): for j in range(W): smImga[i, j, 0] = (pixa[i, j] - mina) * ka smImga[i, j, 1] = smImga[i, j, 0] smImga[i, j, 2] = smImga[i, j, 0] smImgb[i, j, 0] = (pixb[i, j] - minb) * kb smImgb[i, j, 1] = smImgb[i, j, 0] smImgb[i, j, 2] = smImgb[i, j, 0]# 加上拉普拉斯算子# pixa和pixb里面就是两个算子的结果# lapa和lapb是原图加算子的结果，用来裁剪或者scale的原始数据lapa = np.zeros((H, W), np.int32)lapb = np.zeros((H, W), np.int32)# 缩放处理# maxa = 0# maxb = 0# mina = 255# minb = 255for i in range(H): for j in range(W): lapa[i, j] = img[i, j, 0] - pixa[i, j] lapb[i, j] = img[i, j, 0] - pixb[i, j] # 裁剪处理 if lapa[i, j] &gt; 255: lapa[i, j] = 255 if lapa[i, j] &lt; 0: lapa[i, j] = 0 if lapb[i, j] &gt; 255: lapb[i, j] = 255 if lapb[i, j] &lt; 0: lapb[i, j] = 0 # 缩放处理 # if lapa[i, j] &gt; maxa: # maxa = lapa[i, j] # if lapa[i, j] &lt; mina: # mina = lapa[i, j] # if lapb[i, j] &gt; maxb: # maxb = lapb[i, j] # if lapb[i, j] &lt; minb: # minb = lapb[i, j]# 缩放处理# ka = 0# kb = 0# if maxa &gt; mina:# ka = 255 / maxa# if maxb &gt; minb:# kb = 255 / maxb# scale处理for i in range(H): for j in range(W): # 裁剪处理 imga[i, j, 0] = lapa[i, j] imga[i, j, 1] = lapa[i, j] imga[i, j, 2] = lapa[i, j] imgb[i, j, 0] = lapb[i, j] imgb[i, j, 1] = lapb[i, j] imgb[i, j, 2] = lapb[i, j] # 缩放处理 # if lapa[i, j] &gt; 0: # imga[i, j, 0] = lapa[i, j] * ka # else: # imga[i, j, 0] = 0 # imga[i, j, 1] = imga[i, j, 0] # imga[i, j, 2] = imga[i, j, 0] # if lapb[i, j] &gt; 0: # imgb[i, j, 0] = lapb[i, j] * kb # else: # imgb[i, j, 0] = 0 # imgb[i, j, 1] = imgb[i, j, 0] # imgb[i, j, 2] = imgb[i, j, 0]# 原图plt.subplot(1, 4, 1)plt.axis('off')plt.title('Original image')plt.imshow(img)# 图3.37a的模板plt.subplot(2, 4, 2)plt.axis('off')plt.title('Before sale a')plt.imshow(mImgae)# scale后图3.37a的模板plt.subplot(2, 4, 3)plt.axis('off')plt.title('After sale a')plt.imshow(smImga)# 图3.37a的模板锐化后的图像plt.subplot(2, 4, 4)plt.axis('off')plt.title('Sharpened Image a')plt.imshow(imga)# 图3.37b的模板plt.subplot(2, 4, 6)plt.axis('off')plt.title('Before sale b')plt.imshow(mImgbe)# scale后图3.37b的模板plt.subplot(2, 4, 7)plt.axis('off')plt.title('After sale b')plt.imshow(smImgb)# 图3.37b的模板锐化后的图像plt.subplot(2, 4, 8)plt.axis('off')plt.title('Sharpened Image b')plt.imshow(imgb)plt.show()","categories":[],"tags":[]},{"title":"吴恩达机器学习第三章【Linear Algebra Revie】（线性代数回顾）","slug":"吴恩达机器学习第三章【Linear Algebra Revie】","date":"2020-02-06T04:58:30.386Z","updated":"2020-02-06T04:58:30.386Z","comments":true,"path":"2020/02/06/吴恩达机器学习第三章【Linear Algebra Revie】/","link":"","permalink":"http://uscair.club/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%90Linear%20Algebra%20Revie%E3%80%91/","excerpt":"","text":"Matrices and Vectors【矩阵和向量】在$\\begin{bmatrix}{1402}&amp;{191}\\\\{1371}&amp;{821}\\\\{949}&amp;{1437}\\\\{147}&amp;{1448}\\end{bmatrix}$中，这是一个这个是4×2矩阵，即4行2列，如$m$为行，$n$为列，那么$m×n$即4×2，其中$A_{ij}$指第$i$行，第$j$列的元素。 向量是一种特殊的矩阵，向量一般都是列向量，如：$y=\\left[ \\begin{matrix} {460} \\\\ {232} \\\\ {315} \\\\ {178} \\\\\\end{matrix} \\right]$为四维列向量（4×1）。 Addition and Scalar Multiplication【加法和标量乘法】矩阵的加法：行列数相等的各元素相加。 $\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}+\\begin{bmatrix}{4}&amp;{0.5}\\\\{2}&amp;{5}\\\\{0}&amp;{1}\\end{bmatrix}=\\begin{bmatrix}{5}&amp;{0.5}\\\\{4}&amp;{10}\\\\{3}&amp;{2}\\end{bmatrix}$ 矩阵的数乘：每个元素都要乘。 $3\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}=\\begin{bmatrix}{3}&amp;{0}\\\\{6}&amp;{15}\\\\{9}&amp;{3}\\end{bmatrix}=\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}3$ Matrix Vector Multiplication【矩阵向量乘法】 Matrix Matrix Multiplication【矩阵乘法】矩阵乘法： $m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。 在单变量线性回归中的应用 Matrix Multiplication Properties【矩阵乘法的性质】矩阵乘法的性质： 矩阵的乘法不满足交换律：$A×B≠B×A$ 矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$ 单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示,从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0 $A{{A}^{-1}}={{A}^{-1}}A=I$ 对于单位矩阵，有$AI=IA=A$ Inverse and Transpose【逆、转置】矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$A{{A}^{-1}}={{A}^{-1}}A=I$ 矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$ 定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记${{A}^{T}}=B$。(有些书记为A’=B） ${{\\begin{bmatrix} a& b \\\\ c& d \\\\ e& f \\\\\\end{bmatrix} }^{T}}=\\begin{bmatrix} a&amp; c &amp; e \\\\ b&amp; d &amp; f \\\\\\end{bmatrix}$ 矩阵的转置基本性质: $ {{\\left( A\\pm B \\right)}^{T}}={{A}^{T}}\\pm {{B}^{T}} $${{\\left( A\\times B \\right)}^{T}}={{B}^{T}}\\times {{A}^{T}}$${{\\left( {{A}^{T}} \\right)}^{T}}=A $${{\\left( KA \\right)}^{T}}=K{{A}^{T}} $","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"test","slug":"test","permalink":"http://uscair.club/tags/test/"}],"author":{"name":"www"}},{"title":"吴恩达机器学习第二章【Linear Regression with One Variable】（单变量线性回归）","slug":"Linear Regression with One Variable","date":"2020-02-06T04:58:25.879Z","updated":"2020-02-06T04:58:25.879Z","comments":true,"path":"2020/02/06/Linear Regression with One Variable/","link":"","permalink":"http://uscair.club/2020/02/06/Linear%20Regression%20with%20One%20Variable/","excerpt":"","text":"Model Representation【模型表示】在一个房价预测中，我们根据房屋大小的面积来估计房价，诺房屋面积与价格满足以下关系 由图可知为监督学习，诺一种可能的表达方式为：$h\\theta \\left( x \\right)=\\theta{0} + \\theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归(Linear Regression with One Variable)问题。 图为用单变量线性回归模型来预测面积为1250的房价 我们将要用来描述这个回归问题的标记如下: $m$ 代表训练集中实例的数量 $x$ 代表特征/输入变量 $y$ 代表目标变量/输出变量 $\\left( x,y \\right)$ 代表训练集中的实例 $({{x}^{(i)}},{{y}^{(i)}})$ 代表第$i$ 个观察实例 $h$ 代表学习算法的解决方案或函数也称为假设（hypothesis） Cost Function【代价函数】我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。 在单变量线性回归中，即使得代价函数(Cost Function) $J \\left( \\theta0, \\theta_1 \\right) = \\frac{1}{2m}\\sum\\limits{i=1}^m \\left( h_{\\theta}(x^{(i)})-y^{(i)} \\right)^{2}$最小。 Cost Function - Intuition I【代价函数的直观理解I】 在单变量线性回归中，存在点$(1,1),(2,2),(3,3)$，在$\\theta_0=0$的情况下，表达式为$h(x)=\\theta_1x$来估计 $\\theta_1=1$时： $\\theta_1=\\frac{1}{2}$时： $\\theta_1=0$时： 描绘$J(\\theta_1)-\\theta_1$图得： Cost Function - Intuition II【代价函数的直观理解II】在$J \\left( \\theta_0, \\theta_1 \\right)$、$\\theta_0$与$\\theta_1$的三维图中 可知存在使得$J(\\theta_0,\\theta_1)$最小的$\\theta_0和\\theta_1$ 利用等高图表示 Gradient Descent【梯度下降】梯度下降(Gradient Descent)背后的思想是：开始时我们随机选择一个参数的组合$\\left( {\\theta{0}},{\\theta{1}},……,{\\theta_{n}} \\right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 即$Min\\sum_{\\theta_1、\\theta_2、\\theta_3···\\theta_n}J(\\theta_1,\\theta_2,\\theta_3···\\theta_n)$ 批量梯度下降（batch gradient descent）算法的公式为： 其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。其中:=相当于计算机中的赋值号。 Gradient Descent Intuition【梯度下降的直观理解】梯度下降算法如下： ${\\theta{j}}:={\\theta{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)$ 描述：对$\\theta $赋值，使得$J\\left( \\theta \\right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大 其中$\\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)$表示切线的斜率 让我们来看看如果$a$太小或$a$太大会出现什么情况： 如果$a$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。 如图： 如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。 如图： 假设你将${\\theta{1}}$初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得${\\theta{1}}$不再改变，也就是新的${\\theta{1}}$等于原来的${\\theta{1}}$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$a$保持不变时，梯度下降也可以收敛到局部最低点。 首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，${\\theta_{1}}$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。 Gradient Descent For Linear Regression【梯度下降的线性回归】 $\\frac{\\partial }{\\partial {{\\theta }_{j}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{\\partial }{\\partial {{\\theta }_{j}}}\\frac{1}{2m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}^{2}}$ $j=0$ 时：$\\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}}$ $j=1$ 时：$\\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}$ 则算法改写成： Repeat { ​ ${\\theta{0}}:={\\theta{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{ \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}$ ​ ${\\theta{1}}:={\\theta{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}$ ​ } 在梯度下降中会一步步逼近代价函数最小的值","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"test","slug":"test","permalink":"http://uscair.club/tags/test/"}],"author":{"name":"www"}},{"title":"吴恩达机器学习第一章【Introduction】","slug":"Introduction","date":"2020-02-06T04:58:21.048Z","updated":"2020-02-06T04:58:21.048Z","comments":true,"path":"2020/02/06/Introduction/","link":"","permalink":"http://uscair.club/2020/02/06/Introduction/","excerpt":"","text":"Machine Learning机器学习所研究的主要内容，是关于计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm），有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型；面对新的情况时，模型会给我们提供相应的判断。其中从数据中学得模型的过程称为称为“训练”（training）或“学习”（learning） 123graph LR机器学习--有标记信息--&gt;监督学习机器学习--无标记信息--&gt;无监督学习 机器学习的目标是使学得的模型能很好的适应于“新样本”，而不是在新样本上工作的好好的，学得的模型适用于新样本的能力，称为“泛化”能力。 Supervised Learning【监督学习】定义：根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。也就是说，在监督学习中训练数据==既有特征(feature)又有标签(label)==，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。 监督学习的分类：回归(Regression）、分类（Classification) 回归问题：针对连续型问题 回归通俗一点就是，对已经存在的点（训练数据）进行分析，拟合出适当的函数模型y=f(x)，这里y就是数据的标签，而对于一个新的自变量x，通过这个函数模型得到标签y。 分类问题：针对离散型问题 Unsupervised Learning【无监督学习】定义：我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。 可以这么说，比起监督学习，无监督学习更像是自学，让机器学会自己做事情，是没有标签（label）的。我们只是给定了一组数据，我们的目标是发现这组数据中的特殊结构。例如我们使用无监督学习算法会将这组数据分成两个不同的簇,，这样的算法就叫聚类算法。 Answer : B、C","categories":[{"name":"test","slug":"test","permalink":"http://uscair.club/categories/test/"}],"tags":[{"name":"test","slug":"test","permalink":"http://uscair.club/tags/test/"}],"author":{"name":"www"}},{"title":"Hello World","slug":"hello-world","date":"2020-01-13T08:55:07.598Z","updated":"2020-01-13T08:55:07.598Z","comments":true,"path":"2020/01/13/hello-world/","link":"","permalink":"http://uscair.club/2020/01/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"test","slug":"test","permalink":"http://uscair.club/categories/test/"}],"tags":[{"name":"test","slug":"test","permalink":"http://uscair.club/tags/test/"}]},{"title":"OpenCV—Python:（一）图片的读取、显示、保存","slug":"opencv-read","date":"2019-08-18T09:34:16.000Z","updated":"2019-08-18T09:34:16.000Z","comments":true,"path":"2019/08/18/opencv-read/","link":"","permalink":"http://uscair.club/2019/08/18/opencv-read/","excerpt":"这篇文章描述了使用OpenCV进行图片的读取、显示、保存的方法。","text":"这篇文章描述了使用OpenCV进行图片的读取、显示、保存的方法。 图片的阅读、显示、保存：阅读：cv2.imread(“图片的路径”, 打开方式) 图片路径： 绝对路径：与代码不在同一目录里，写出详细地址，如“E:\\\\Photos\\\\aobing.jpg” 相对路径:代码相同目录中，只需写出图片名即可，如：“ao_bing.jpg” 注：不知道图片路径时，pycharm支持查看：点击图片文件名，单机单击鼠标右键， “Copy Path” 是绝对路径，”Copy Relative Path” 是相对路径! 打开方式： “ 1 “， 默认打开方式，加载彩色图像，任何图像的透明度都将忽略 “ 0 “， 加载灰度图像 “ -1 “，加载图像，包括alpha通道 显示：cv2.imshow(“窗口名称”，图片) 窗口名称：自己命名（不同的窗口使用不同的名称，但可以是同一张图片） 图片：打开图片时自己命名的变量,如 12#img即图片img=cv2.imread(\"ao_bing.jpg\") 保存：cv2.imwrite(“文件名”,要保存的图像) 总结：完整代码如下： 123456789101112131415161718192021222324#-*- coding:utf-8 -*-#也可以 import cv2 as cv ,使用时用cv代替cv2import cv2#宏定义文件名，便于修改filename=\"ao_bing.jpg\" #这里取的是相对路径#读入图片img=cv2.imread(filename) #默认打开，彩色图像img_gray=cv2.imread(filename,0)#灰度图打开，黑白图像#显示图片cv2.imshow(\"Img\",img)cv2.imshow(\"Img_gray\",img_gray)#使图片长时间停留，不闪退cv2.waitKey(0)#保存图片cv2.imwrite(\"ao_bing_gray.jpg\",img_gray)#摧毁所有窗口cv2.destroyAllWindows()","categories":[{"name":"数字图像处理","slug":"数字图像处理","permalink":"http://uscair.club/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://uscair.club/tags/OpenCV/"}],"author":{"name":"Adans"}}]}