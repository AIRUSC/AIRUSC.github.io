{"meta":{"title":"AIR","subtitle":"A research team studying artificial intelligence","description":"AIR","author":"AIR@USC","url":"http://uscair.club","root":"/"},"pages":[{"title":"关于","date":"2020-02-06T08:22:15.191Z","updated":"2020-02-06T08:22:15.191Z","comments":true,"path":"about/index.html","permalink":"http://uscair.club/about/index.html","excerpt":"","text":"AIR@USCContact Address: the University of South China - 28 Changsheng West Road- Hunan, China E-mail: airusc@foxmail.com QQ：3458038461 Blog: https://uscair.club Introduction:A professional team studying artificial intelligence and robotics Instructor:Dr. Mao Yu Honor One First Prize &amp; Two Third Prizes, [The 15th Hunan University Student Computer Program Design Competition] Aug. 2019 One Second Prize &amp; Three Third Prizes,[“Soft Silver Robot Cup” China robot skill competition] Dec. 2018 EssayGrade 16 Qiu Zhongxi published EI paper as first author"},{"title":"所有标签","date":"2020-02-06T08:17:31.100Z","updated":"2020-02-06T08:17:31.100Z","comments":true,"path":"tags/index.html","permalink":"http://uscair.club/tags/index.html","excerpt":"","text":""},{"title":"所有分类","date":"2020-02-06T08:16:28.355Z","updated":"2020-02-06T08:16:28.355Z","comments":true,"path":"categories/index.html","permalink":"http://uscair.club/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"关于numpy矩阵运算的小记","slug":"关于numpy矩阵运算的小记","date":"2020-02-15T16:00:00.000Z","updated":"2020-02-15T16:00:00.000Z","comments":true,"path":"2020/02/16/关于numpy矩阵运算的小记/","link":"","permalink":"http://uscair.club/2020/02/16/%E5%85%B3%E4%BA%8Enumpy%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%9A%84%E5%B0%8F%E8%AE%B0/","excerpt":"","text":"关于numpy矩阵运算的小记[toc] 发现做矩阵处理是numpy忘了好多，所以记录下来. array与matrixmatrix是array的分支，matrix和array在很多时候都是通用的，你用哪一个都一样。但这时候，官方建议大家如果两个可以通用，那就选择array，因为array更灵活，速度更快，很多人把二维的array也翻译成矩阵。但是matrix的优势就是相对简单的运算符号，比如两个矩阵相乘，就是用符号*，但是array相乘不能这么用，得用方法.dot()array的优势就是不仅仅表示二维，还能表示3、4、5…维，而且在大部分Python程序里，array也是更常用的。 123456789101112import numpy as npa1 = np.array([[1, 2], [3, 4]])a2 = np.array([[5, 6], [7, 8]])b1 = np.mat([[1, 2], [3, 4]])b2 = np.mat([[5, 6], [7, 8]])print((np.dot(a1, a2)).all() == (b1 * b2).all()) 输出 True 矩阵判等在矩阵判等中存在各个元素相等和整个矩阵相等两种情况 1234567891011121314import numpy as npa1 = np.array([[1, 2], [3, 4]])a2 = np.array([[5, 6], [7, 8]])b1 = np.mat([[1, 2], [3, 4]])b2 = np.mat([[5, 6], [7, 8]])print('np.dot(a1, a2):\\n', np.dot(a1, a2))print('b1 * b2:\\n', b1 * b2)print((np.dot(a1, a2)) == (b1 * b2))print((np.dot(a1, a2)).all() == (b1 * b2).all()) 输出： 其中==用于判等各个元素相等，用all()方法可以判断整个矩阵相等 矩阵的连接在numpy中存在按行连接与按列连接两种形式 1234567891011121314151617import numpy as npx = np.array([[1, 2, 3], [4, 5, 6]])print(\"x:\\n\", x)y = np.array([[7, 8, 9], [10, 11, 12]])print(\"y:\\n\", y)z = np.array([[13, 14, 15]])print(\"z:\\n\", z)a = np.vstack((x, y, z))print('np.vstack((x,y,z)):\\n', a)b = np.hstack((x, y))print('np.hstack((x,y)):\\n', b) 输出： 其中np.hstack()是按行连接：行数相同的的连接在一起；np.vstack()按列连接：列书相同的连接在一起。 矩阵的向量化在矩阵中我们可以使用reshape方法来实现矩阵的向量化，如果在整形操作中将尺寸标注为-1，则会自动计算其他尺寸 1234567891011121314151617181920import numpy as npx = np.array([[1, 2, 3], [4, 5, 6]])print(\"x:\\n\", x)y = np.array([[7, 8, 9], [10, 11, 12]])print(\"y:\\n\", y)z = np.array([[13, 14, 15]])print(\"z:\\n\", z)a = np.vstack((x, y, z))print('np.vstack((x,y,z)):\\n', a)b = np.hstack((x, y))print('np.hstack((x,y)):\\n', b)print(a.shape)print(a.reshape(-1,1)) 输出： 矩阵的拆分使用hsplit，您可以沿数组的水平轴拆分数组，方法是指定要返回的形状相同的数组的数量，或者指定要在其后进行划分的列，使用vsplit，您可以沿数组的竖直轴拆分数组，方法是指定要返回的形状相同的数组的数量，或者指定要在其后进行划分的列。 12345678import numpy as npa = np.array([[1, 1, 2, 3], [1, 5, 7, 3], [7, 3, 9, 3], [1, 7, 3, 0]])print(np.hsplit(a, 2))print(np.vsplit(a, 2)) 矩阵的复制用=的简单分配不会复制数组对象或其数据，该copy方法对数组及其数据进行完整复制。 12345678910111213import numpy as npa = np.array([[1, 1, 2, 3], [1, 5, 7, 3], [7, 3, 9, 3], [1, 7, 3, 0]])# print(np.hsplit(a, 2))# print(np.vsplit(a, 2))b = aprint(id(a))print(id(b))c = a.copy()print(id(c)) 矩阵的逆 利用numpy.linalg.inv()可以求得矩阵的逆矩阵 123456789import numpy as npa = np.array([[1, 1, 2, 3], [1, 5, 7, 3], [7, 3, 9, 3], [1, 7, 3, 0]])b = np.linalg.inv(a)print(np.dot(a, b)) 矩阵的索引12345678910111213import numpy as npa = np.array([[1, 1, 2, 3], [1, 5, 7, 3], [7, 3, 9, 3], [1, 7, 3, 0]])print(a[2, 3]) # 输出第2+1行第3+1列的数字print(a[1:]) # 以矩阵形式输出从第1+1行开始的所有数print(a[1:3]) # 以矩阵形式输出第1+1到第3+1-1行的数print(a[:, 1]) # 输出矩阵的第1+1列的所有数print(a[0:3, 1]) # 输出矩阵从0+1行到第3+1-1行的第1+1列的所有数print(type(a[:3]))","categories":[{"name":"python基础","slug":"python基础","permalink":"http://uscair.club/categories/python%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Numpy","slug":"Numpy","permalink":"http://uscair.club/tags/Numpy/"}],"author":{"name":"Gowi"}},{"title":"神经网络--反向传播算法推导","slug":"神经网络--反向传播算法推导","date":"2020-02-12T16:00:00.000Z","updated":"2020-02-12T16:00:00.000Z","comments":true,"path":"2020/02/13/神经网络--反向传播算法推导/","link":"","permalink":"http://uscair.club/2020/02/13/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C--%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/","excerpt":"","text":"概述以监督学习为例，假设我们有训练样本集$(x^{(i)},y^{(i)})$，那么神经网络算法能提供一种复杂且非线性的假设模型$h_{(W,b)}(x)$,它具有参数$W$，$b$，可以以此参数来拟合我们的数据。 为了描述神经网络，我们先从最简单的神经网络讲起，这个神经网络仅由一个“神经元”构成，以下即是这个“神经元”的图示： 这个神经元是一个以$x_1,x_2,x_3$以及截距+1为输入值的运算单元，其输出为$h_{(W,b)}(x)=f(W^Tx)=f(\\overset{3}{\\underset{i=1}{\\sum}}W_iX_i+b)$,其中函数$f:\\mathbb{R}\\to\\mathbb{R}$被称为激活函数 我们选用$sigmoid$函数为激活函数$$f(z)=\\frac{1}{1+e^{-z}}$$可以看出，这个单一“神经元”的输入－输出映射关系其实就是一个逻辑回归（logistic regression）。 我们采用$sigmoid$函数 其中$f’(z)=f(z)(1-f(z))$,如果是$tanh$函数，则$f’(z)=1-(f(z))^2$。 神经网络模型所谓神经网络就是将许多个单一“神经元”联结在一起，这样，一个“神经元”的输出就可以是另一个“神经元”的输入。例如，下图就是一个简单的神经网络： 我们使用圆圈来表示神经网络的输入，标上“+1”的圆圈被称为偏置节点，也就是截距项。神经网络最左边的一层叫做输入层，最右的一层叫做输出层（本例中，输出层只有一个节点）。中间所有节点组成的一层叫做隐藏层，因为我们不能在训练样本集中观测到它们的值。同时可以看到，以上神经网络的例子中有3个输入单元（偏置单元不计在内），3个隐藏单元及一个输出单元。 我们用$n_l$来表示网络的层数，本例中 $\\textstyle n_l=3$ ，我们将第 $\\textstyle l$ 层记为 $\\textstyle L_l$ ，于是$\\textstyle L_1$ 是输入层，输出层是 $\\textstyle L_{n_l}$ 。本例神经网络有参数 $\\textstyle (W,b) = (W^{(1)}, b^{(1)}, W^{(2)}, b^{(2)})$ ，其中$\\textstyle W^{(l)}_{ij}$（下面的式子中用到）是第 $\\textstyle l$ 层第 $\\textstyle j$ 单元与第 $\\textstyle l+1$ 层第 $i$ 单元之间的联接参数（其实就是连接线上的权重，注意标号顺序）， $\\textstyle b^{(l)}_i$ 是第 $\\textstyle l+1$ 层第$i$单元的偏置项。因此在本例中， $\\textstyle W^{(1)} \\in \\Re^{3\\times 3}$ ， $\\textstyle W^{(2)} \\in \\Re^{1\\times 3}$ 。注意，没有其他单元连向偏置单元(即偏置单元没有输入)，因为它们总是输出 $\\textstyle +1$。同时，我们用 $\\textstyle s_l$ 表示第 $\\textstyle l$ 层的节点数（偏置单元不计在内）。 我们用 $\\textstyle a^{(l)}_i$ 表示第 $\\textstyle l$层第 $\\textstyle i$ 单元的激活值（输出值）。当$\\textstyle l=1$ 时，$ \\textstyle a^{(1)}_i = x_i $，也就是第 $\\textstyle i$ 个输入值（输入值的第$\\textstyle i$个特征)。对于给定参数集合 $\\textstyle W,b$，我们的神经网络就可以按照函数 $\\textstyle h_{W,b}(x)$ 来计算输出结果。本例神经网络的计算步骤如下：$$a_1^{(2)}=f(W_{11}^{(1)}x_1+W_{12}^{(1)}x_2+W_{13}^{(1)}x_3+b_1^{(1)})\\a_1^{(2)}=f(W_{21}^{(1)}x_1+W_{22}^{(1)}x_2+W_{23}^{(1)}x_3+b_1^{(1)})\\a_1^{(2)}=f(W_{31}^{(1)}x_1+W_{32}^{(1)}x_2+W_{33}^{(1)}x_3+b_1^{(1)})\\h_{W,b}(x)=a_1^{(3)}=f(W_{11}^{(2)}a_1^{(2)}+W_{12}^{(2)}a_2^{(2)}+W_{13}^{(2)}a_3^{(2)}+b_1^{(2)})$$我们用 $\\textstyle z^{(l)}_i$ 表示第$\\textstyle l$ 层第 $\\textstyle i$ 单元输入加权和（包括偏置单元），比如， $\\textstyle z_i^{(2)} = \\sum_{j=1}^n W^{(1)}_{ij} x_j + b^{(1)}_i$ ，则 $\\textstyle a^{(l)}_i = f(z^{(l)}_i)$ 。 这样我们就可以得到一种更简洁的表示法。这里我们将激活函数 $\\textstyle f(\\cdot)$ 扩展为用向量（分量的形式）来表示，即 $\\textstyle f([z_1, z_2, z_3]) = [f(z_1), f(z_2), f(z_3)]$，那么，上面的等式可以更简洁地表示为：$$z^{(2)}=W^{(1)}x+b^{(1)}\\a^{(2)}=f(z^{(2)})\\z^{(3)}=W^{(2)}x+b^{(2)}\\h_{W,b}(x)=a^{(3)}=f(z^{(3)})$$我们将上面的计算步骤叫作前向传播。回想一下，之前我们用 $\\textstyle a^{(1)} = x$ 表示输入层的激活值，那么给定第 $\\textstyle l$的激活值 $\\textstyle a^{(l)}$ 后，第 $\\textstyle l+1$ 层的激活值 $\\textstyle a^{(l+1)}$ 就可以按照下面步骤计算得到：$$z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}\\a^{(l+1)}=f(z^{(l+1)})$$将参数矩阵化，使用矩阵－向量运算方式，我们就可以利用线性代数的优势对神经网络进行快速求解。 目前为止，我们讨论了一种神经网络，我们也可以构建另一种结构的神经网络（这里结构指的是神经元之间的联接模式），也就是包含多个隐藏层的神经网络。最常见的一个例子是 $\\textstyle n_l$ 层的神经网络，第 $\\textstyle 1$ 层是输入层，第 $\\textstyle n_l$ 层是输出层，中间的每个层 $\\textstyle l$ 与层 $\\textstyle l+1$ 紧密相联。这种模式下，要计算神经网络的输出结果，我们可以按照之前描述的等式，按部就班，进行前向传播，逐一计算第 $\\textstyle L_2$ 层的所有激活值，然后是第 $\\textstyle L_3$ 层的激活值，以此类推，直到第 $\\textstyle L_{n_l}$层。这是一个前馈神经网络的例子，因为这种联接图没有闭环或回路。 神经网络也可以有多个输出单元。比如，下面的神经网络有两层隐藏层： $\\textstyle L_2$ 及 $\\textstyle L_3$ ，输出层 $\\textstyle L_4$ 有两个输出单元。 要求解这样的神经网络，需要样本集 $\\textstyle (x^{(i)}, y^{(i)})$ ，其中 $\\textstyle y^{(i)} \\in \\Re^2$ 。如果你想预测的输出是多个的，那这种神经网络很适用。（比如，在医疗诊断应用中，患者的体征指标就可以作为向量的输入值，而不同的输出值$\\textstyle y_i$可以表示不同的疾病存在与否。） 反向传导算法假设我们有一个固定样本集 $\\textstyle \\{ (x^{(1)}, y^{(1)}), \\ldots, (x^{(m)}, y^{(m)}) \\}$，它包含 $\\textstyle m$ 个样例。我们可以用批量梯度下降法来求解神经网络。具体来讲，对于单个样例 $\\textstyle (x,y)$)，其代价函数为：$$\\begin{align}J(W,b; x,y) = \\frac{1}{2} \\left| h_{W,b}(x) - y \\right|^2.\\end{align}$$这是一个（二分之一的）方差代价函数。给定一个包含 $\\textstyle m$ 个样例的数据集，我们可以定义整体代价函数为：$$\\begin{align}J(W,b)&amp;= \\left[ \\frac{1}{m} \\sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \\right] + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2 \\\\&amp;= \\left[ \\frac{1}{m} \\sum_{i=1}^m \\left( \\frac{1}{2} \\left| h_{W,b}(x^{(i)}) - y^{(i)} \\right|^2 \\right) \\right] + \\frac{\\lambda}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( W^{(l)}_{ji} \\right)^2\\end{align}$$以上关于$\\textstyle J(W,b)$定义中的第一项是一个均方差项。第二项是一个规则化项（也叫权重衰减项），其目的是减小权重的幅度，防止过度拟合。 [注：通常权重衰减的计算并不使用偏置项 $\\textstyle b^{(l)}_i$，比如我们在 $\\textstyle J(W, b)$ 的定义中就没有使用。一般来说，将偏置项包含在权重衰减项中只会对最终的神经网络产生很小的影响。在贝叶斯规则化方法中，我们将高斯先验概率引入到参数中计算MAP（极大后验）估计（而不是极大似然估计）。] 权重衰减参数 $\\textstyle \\lambda$ 用于控制公式中两项的相对重要性。在此重申一下这两个复杂函数的含义：$\\textstyle J(W,b;x,y)$ 是针对单个样例计算得到的方差代价函数；$\\textstyle J(W,b)$ 是整体样本代价函数，它包含权重衰减项。 以上的代价函数经常被用于分类和回归问题。在分类问题中，我们用 $\\textstyle y = 0$ 或 $\\textstyle 1$，来代表两种类型的标签（回想一下，这是因为 sigmoid激活函数的值域为 !$\\textstyle [0,1]$；如果我们使用双曲正切型激活函数，那么应该选用 $\\textstyle -1$ 和 $\\textstyle +1$ 作为标签）。对于回归问题，我们首先要变换输出值域（也就是$y$），以保证其范围为 $\\textstyle [0,1]$ （同样地，如果我们使用双曲正切型激活函数，要使输出值域为 $\\textstyle [-1,1]$）。 我们的目标是针对参数 $\\textstyle W$ 和 $\\textstyle b$ 来求其函数 $\\textstyle J(W,b)$ 的最小值。为了求解神经网络，我们需要将每一个参数 $\\textstyle W^{(l)}_{ij}$ 和 $\\textstyle b^{(l)}_i$ 初始化为一个很小的、接近零的随机值（比如说，使用正态分布 $\\textstyle {Normal}(0,\\epsilon^2)$ 生成的随机值，其中 $\\textstyle \\epsilon$ 设置为 $\\textstyle 0.01$ ），之后对目标函数使用诸如批量梯度下降法的最优化算法。因为 $\\textstyle J(W, b)$ 是一个非凸函数，梯度下降法很可能会收敛到局部最优解；但是在实际应用中，梯度下降法通常能得到令人满意的结果。最后，需要再次强调的是，要将参数进行随机初始化，而不是全部置为$\\textstyle 0$。如果所有参数都用相同的值作为初始值，那么所有隐藏层单元最终会得到与输入值有关的、相同的函数（也就是说，对于所有 $\\textstyle i$，$\\textstyle W^{(1)}_{ij}$都会取相同的值，那么对于任何输入 $\\textstyle x$ 都会有：$\\textstyle a^{(2)}_1 = a^{(2)}_2 = a^{(2)}_3 = \\ldots$ ）。随机初始化的目的是使对称失效。 梯度下降法中每一次迭代都按照如下公式对参数 $\\textstyle W$ 和$\\textstyle b$ 进行更新：$$\\begin{align}W_{ij}^{(l)} &amp;= W_{ij}^{(l)} - \\alpha \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b) \\\\b_{i}^{(l)} &amp;= b_{i}^{(l)} - \\alpha \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b)\\end{align}$$其中 $\\textstyle \\alpha$ 是学习速率。其中关键步骤是计算偏导数。我们现在来讲一下反向传播算法，它是计算偏导数的一种有效方法。 我们首先来讲一下如何使用反向传播算法来计算 $\\textstyle \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y)$ 和 $\\textstyle \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y)$，这两项是单个样例 $\\textstyle (x,y)$ 的代价函数 $\\textstyle J(W,b;x,y)$ 的偏导数。一旦我们求出该偏导数，就可以推导出整体代价函数$\\textstyle J(W,b)$的偏导数：$$\\begin{align}\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b) &amp;=\\left[ \\frac{1}{m} \\sum_{i=1}^m \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x^{(i)}, y^{(i)}) \\right] + \\lambda W_{ij}^{(l)} \\\\\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b) &amp;=\\frac{1}{m}\\sum_{i=1}^m \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x^{(i)}, y^{(i)})\\end{align}$$以上两行公式稍有不同，第一行比第二行多出一项，是因为权重衰减是作用于 $\\textstyle W$ 而不是$\\textstyle b$。 反向传播算法的思路如下：给定一个样例 $\\textstyle (x,y)$，我们首先进行“前向传导”运算，计算出网络中所有的激活值，包括 $\\textstyle h_{W,b}(x)$ 的输出值。之后，针对第 $\\textstyle l$ 层的每一个节点 $\\textstyle i$，我们计算出其“残差” $\\textstyle \\delta^{(l)}_i$，该残差表明了该节点对最终输出值的残差产生了多少影响。对于最终的输出节点，我们可以直接算出网络产生的激活值与实际值之间的差距，我们将这个差距定义为 $\\textstyle \\delta^{(n_l)}_i$ （第 $\\textstyle n_l$ 层表示输出层）。对于隐藏单元我们如何处理呢？我们将基于节点（第 $\\textstyle l+1$ 层节点）残差的加权平均值计算 $\\textstyle \\delta^{(l)}_i$，这些节点以 $\\textstyle a^{(l)}_i$ 作为输入。下面将给出反向传导算法的细节： 进行前馈传导计算，利用前向传导公式，得到 $\\textstyle L_2, L_3, \\ldots$ 直到输出层 $\\textstyle L_{n_l}$ 的激活值。 对于第 $n_l$（输出层）的每个输出单元$i$我们根据以下公式计算残差： 对 $\\textstyle l = n_l-1, n_l-2, n_l-3, \\ldots, 2$ 的各个层，第$\\textstyle l$层的第 $\\textstyle i$ 个节点的残差计算方法如下： $$\\delta_i^{(n_l)}=(\\overset{s_{l+1}}{\\underset{j=1}{\\sum}}W_{ji}^{(l)}\\delta_j^{(l+1)})f’(z_i^{(l)})$$ $$\\begin{align}\\delta^{(n_l-1)}_i &amp;=\\frac{\\partial}{\\partial z^{n_l-1}_i}J(W,b;x,y) = \\frac{\\partial}{\\partial z^{n_l-1}_i}\\frac{1}{2} \\left|y - h_{W,b}(x)\\right|^2 = \\frac{\\partial}{\\partial z^{n_l-1}_i}\\frac{1}{2} \\sum_{j=1}^{S_{n_l}}(y_j-a_j^{(n_l)})^2 \\\\&amp;= \\frac{1}{2} \\sum_{j=1}^{S_{n_l}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j-a_j^{(n_l)})^2 = \\frac{1}{2} \\sum_{j=1}^{S_{n_l}}\\frac{\\partial}{\\partial z^{n_l-1}_i}(y_j-f(z_j^{(n_l)}))^2 \\\\&amp;= \\sum_{j=1}^{S_{n_l}}-(y_j-f(z_j^{(n_l)})) \\cdot \\frac{\\partial}{\\partial z_i^{(n_l-1)}}f(z_j^{(n_l)}) = \\sum_{j=1}^{S_{n_l}}-(y_j-f(z_j^{(n_l)})) \\cdot f’(z_j^{(n_l)}) \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{(n_l-1)}} \\\\&amp;= \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot \\frac{\\partial z_j^{(n_l)}}{\\partial z_i^{n_l-1}} = \\sum_{j=1}^{S_{n_l}} \\left(\\delta_j^{(n_l)} \\cdot \\frac{\\partial}{\\partial z_i^{n_l-1}}\\sum_{k=1}^{S_{n_l-1}}f(z_k^{n_l-1}) \\cdot W_{jk}^{n_l-1}\\right) \\\\&amp;= \\sum_{j=1}^{S_{n_l}} \\delta_j^{(n_l)} \\cdot W_{ji}^{n_l-1} \\cdot f’(z_i^{n_l-1}) = \\left(\\sum_{j=1}^{S_{n_l}}W_{ji}^{n_l-1}\\delta_j^{(n_l)}\\right)f’(z_i^{n_l-1})\\end{align}$$ 将上式中的$\\textstyle n_l-1$与$\\textstyle n_l$的关系替换为$\\textstyle l$与$\\textstyle l+1$的关系，就可以得到：$$\\delta^{(l)}_i = \\left( \\sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \\delta^{(l+1)}_j \\right) f’(z^{(l)}_i)$$以上逐次从后向前求导的过程即为“反向传导”的本意所在。 ​ 4.计算我们需要的偏导数，计算方法如下 ：$$\\begin{align}\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) &amp;= a^{(l)}_j \\delta_i^{(l+1)} \\\\\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) &amp;= \\delta_i^{(l+1)}.\\end{align}\\$$求$\\begin{align}\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W)\\end{align}$?$$\\begin{align}\\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W)&amp;=\\frac{\\partial }{\\partial Z_{ij}^{(l)}}J(W)=\\frac{\\partial J(W)}{\\partial Z_{i}^{(l+1)}}\\times\\frac{\\partial Z_{i}^{(l+1)}}{\\partial Z_{ij}^{(l)}} &amp;(问题拆解)\\Z_i^{(l+1)} &amp;= \\overset{n}{\\underset{i}{\\sum}}W_{ij}^{l}\\times a_j^{(l)}&amp;(神经元求和)\\\\frac{\\partial Z_{i}^{(l+1)}}{\\partial Z_{ij}^{(l)}}&amp;=\\frac{\\partial \\overset{n}{\\underset{i}{\\sum}}W_{ij}^{(l)}\\times a_j^{(l)}}{\\partial W_{ij}^{(l)}}=a_j^{(l)}&amp;(输出对权值的偏导数)\\\\delta_i^{l+1}&amp;=\\frac{\\partial J(W)}{\\partial Z_i^{(l+1)}}&amp;(神经元的错误变化率为)\\\\frac{\\partial}{\\partial W_i^{(l)}} J(W) &amp;= \\delta_i^{l+1}\\times a_j^{(l)}&amp;(最终)\\end{align}$$最后，我们用矩阵-向量表示法重写以上算法。我们使用“$\\textstyle \\bullet$” 表示向量乘积运算符（在Matlab或Octave里用“.*”表示，也称作阿达马乘积）。若$\\textstyle a = b \\bullet c$，则 $\\textstyle a_i = b_ic_i$。在上一个教程中我们扩展了 $\\textstyle f(\\cdot)$ 的定义，使其包含向量运算，这里我们也对偏导数 $\\textstyle f’(\\cdot)$ 也做了同样的处理（于是又有 $\\textstyle f’([z_1, z_2, z_3]) = [f’(z_1), f’(z_2), f’(z_3)]$ ）。 么，反向传播算法可表示为以下几个步骤： 进行前馈传导计算，利用前向传导公式，得到 $\\textstyle L_2, L_3, \\ldots$直到输出层 $\\textstyle L_{n_l}$ 的激活值。 对输出层（第 $n_l$层），计算： $$ \\begin{align}\\delta^{(n_l)}= - (y - a^{(n_l)}) \\bullet f’(z^{(n_l)})\\end{align} $$ 对于$ \\textstyle l = n_l-1, n_l-2, n_l-3, \\ldots, 2$的各层，计算： $$ \\begin{align}\\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\bullet f’(z^{(l)})\\end{align} $$ 计算最终需要的偏导数值： $$ \\begin{align}\\nabla_{W^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)} (a^{(l)})^T, \\\\\\nabla_{b^{(l)}} J(W,b;x,y) &amp;= \\delta^{(l+1)}.\\end{align} $$ 实现中应注意：在以上的第2步和第3步中，我们需要为每一个 $\\textstyle i$ 值计算其 $\\textstyle f’(z^{(l)}_i)$。假设 $\\textstyle f(z)$ 是sigmoid函数，并且我们已经在前向传导运算中得到了 $\\textstyle a^{(l)}_i$。那么，使用我们早先推导出的 !$\\textstyle f’(z)$表达式，就可以计算得到 $\\textstyle f’(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)$。 最后，我们将对梯度下降算法做个全面总结。在下面的伪代码中，$\\textstyle \\Delta W^{(l)}$ 是一个与矩阵 $\\textstyle W^{(l)}$ 维度相同的矩阵，$\\textstyle \\Delta b^{(l)}$ 是一个与 $\\textstyle b^{(l)}$ 维度相同的向量。注意这里“$\\textstyle \\Delta W^{(l)}$”是一个矩阵，而不是“$\\textstyle \\Delta$ 与 $\\textstyle W^{(l)}$ 相乘”。下面，我们实现批量梯度下降法中的一次迭代： 对于所有 $\\textstyle l$，令 $\\textstyle \\Delta W^{(l)} := 0$ , $\\textstyle \\Delta b^{(l)} := 0$ （设置为全零矩阵或全零向量） 对于$i=1$到$m$ 使用反向传播算法计算 $\\textstyle \\nabla_{W^{(l)}} J(W,b;x,y)$ 和 $\\textstyle \\nabla_{b^{(l)}} J(W,b;x,y)$ 计算$\\textstyle \\Delta W^{(l)} := \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;x,y)$ 计算 $\\textstyle \\Delta b^{(l)} := \\Delta b^{(l)} + \\nabla_{b^{(l)}} J(W,b;x,y)$ 更新权重参数 $\\begin{align}W^{(l)} &amp;= W^{(l)} - \\alpha \\left[ \\left(\\frac{1}{m} \\Delta W^{(l)} \\right) + \\lambda W^{(l)}\\right] \\\\b^{(l)} &amp;= b^{(l)} - \\alpha \\left[\\frac{1}{m} \\Delta b^{(l)}\\right]\\end{align}$ 现在，我们可以重复梯度下降法的迭代步骤来减小代价函数 $\\textstyle J(W,b)$ 的值，进而求解我们的神经网络。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"神经网络、机器学习基础","slug":"神经网络、机器学习基础","permalink":"http://uscair.club/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"author":{"name":"Gowi"}},{"title":"机器学习第一章【Introduction】","slug":"机器学习系列（一）——回归模型","date":"2020-02-09T17:33:36.400Z","updated":"2020-02-09T17:33:36.400Z","comments":true,"path":"2020/02/10/机器学习系列（一）——回归模型/","link":"","permalink":"http://uscair.club/2020/02/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"机器学习系列（一）——回归模型回归（Regression）模型是指机器学习方法学到的函数的输出是连续实数值，回归模型可以用于预测或者分类，这篇博客中主要整理用于预测的线性回归模型和多项式回归模型。 线性回归按照机器学习建模的三个步骤，首先需要确定选用的模型，这里就是线性回归（Linear regression）模型，然后将其形式化表达： 其中，x1,x2,⋯,xn是样本数据的n维属性描述，每一组w和b能确定一个不一样的h(x)，w和b的所有取值组合就构成了可选函数集合，我们的任务就是要从这个函数集合中选出“最好”的那个函数。对于训练数据集D描述如下： 其中)是样本的n维特征向量表示，y(i)∈R是样本标记。线性回归的目标是学得一个线性函数以尽可能准确的预测实值输出标记。因此我们需要确定一个衡量标准用以度量一个函数的好坏，也就是选择合适的损失函数（Loss Function）。根据线性回归的目标，我们只需要度量h(x)与y之间的差距，均方误差（Mean Square Error，MSE）是回归任务中最常用的损失函数。 因为hh是关于w,bw,b的函数，所以上式也可以写成能够让LL最小的w,bw,b所确定的函数就是我们要找的最好的那个函数，记为w∗,b∗ 现在需要选择一种优化算法从众多w,b中找出w∗,b∗w∗,b∗，常用的方法有梯度下降算法（Gradient Descent）和最小二乘法（Least Square Method，LSM）。 梯度下降法对多元函数的每一个变量分别求∂∂偏导数，并将结果写成向量形式，就是梯度。比如函数f(x,y)，分别对x,y求偏导数，(∂f/∂x,∂f/∂y)^T就是梯度。从几何意义上讲，梯度是函数在该点变化最快的方向，因此沿着梯度的反方向，会更加容易找到函数的最小值点。下图是梯度下降的一个直观解释 比如我们在一座大山的某个位置想要到达山脚下，由于我们不知道怎么下山，于是决定走一步算一步，每次走到一个位置后，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的方向向下走，直到走到梯度为零的位置。当然这样走下去，有可能不能走到山脚，而是到了某一个局部山峰低处。因此梯度下降不一定能够找到全局最优解，有可能是一个局部最优解，如果损失函数是凸函数，梯度下降算法就一定能找到全局最优解。梯度下降算法步骤如下： 随机选取一个初始值w^0计算，其中ηη是学习速率，决定了每一次调整的步长 迭代第二步多次直至梯度为零，或者损失达到允许范围，或者达到迭代次数。 最小二乘法同样可以采用最小二乘法来对w,b进行估计，最小二乘法直接求解得到解析解。为了方便讨论，我们把w,b合并组成一个向量)，同时给每一个样本增加一个恒定属性值1。同时，把数据集D表示成一个m∗(n+1)m∗(n+1)大小的矩阵X再把标记也写成向量形式y=(y1;y2;⋯;ym)，则类似的有损失函数对w^求导得到令上式等于零，可得w^w^最优解的闭式解其中，(X^TX)−1是(X^TX)的逆矩阵，可能存在(X^TX)不是满秩矩阵，常见的作法是引入正则化（regularization）项。 多项式回归有些样本数据用线性回归拟合时可能不是特别恰当，这时候可以尝试采用多项式回归，如下图所示，是一个房屋价格与房屋面积的样本数据集，可以明显发现所有数据点并不分布在某一条直线附近，这时候可以考虑尝试二次函数或者三次函数 对于多项式回归，可以将x^2,x^3也看作是一个新的属性，这时多项式回归就和线性回归一样了，因此多项式回归的训练方法依旧和线性回归一样。需要注意的是，能够选择简单的线性模型时就不要选择相对复杂的多项式模型。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"http://uscair.club/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"author":{"name":"Wonder"}},{"title":"Nao  机器人入门","slug":"NAO机器人（python语言）","date":"2020-02-09T11:28:56.705Z","updated":"2020-02-09T11:28:56.705Z","comments":true,"path":"2020/02/09/NAO机器人（python语言）/","link":"","permalink":"http://uscair.club/2020/02/09/NAO%E6%9C%BA%E5%99%A8%E4%BA%BA%EF%BC%88python%E8%AF%AD%E8%A8%80%EF%BC%89/","excerpt":"","text":"NAO机器人（python语言） 是一种双足人形机器人，58公分高，可以做出各种与人类一样的 肢体语言，通过代码实现了识别语言并根据代码做出相应的回应，能够通过感应器识别物体，与人类互动，NAO提供了一个独立的编程环境（Choregraphe），需要密匙才能激活对应 机器人环境（到时候会帮助你们建立起来）。 NAO机器人是人工智能机器人，嵌入式软件使得NAO可以声音合成，图像识别，行为控制，双通道超声波探测障碍物，以及自身二极管进行视觉效果。 在C++、Python、Choregraphe中，我们是通过编译代码实现对机器人控制实现，多看NAOqi API ,在那之前我们了解一下NAO机器人的基本情况： 1.NAO能够探测前方0.25-2.55m内是否有障碍物，探测角度60°。 2.触摸、按压、划过接触传感器可以出发接触传感器产生电信号，进而完成向机器人输入信息.头部：前中后三个触摸传感器。手部触摸传感器，脚前部的碰撞传感器（也起到缓冲作用）。胸前：长按、短按、连按。开机：按胸前按钮。报IP地址：在开机并且联网状态下，按胸前按钮。关机：在开机状态下，按胸前按钮。自主生活状态：连续按两次胸前按钮进入或退出。 3.测量身体状态以及加速度，包括2个陀螺仪，1个加速度计。4.（MRE磁性编码器）：测量机器人自身关节位置，36个。5.每只脚上有4个压力传感器，用来确定每只脚重心的位置。在行走过程中，NAO根据重心为在进行不太调整以保持身体平衡。6.头部传感器周围：12个LED耳部：2的10次方个16级蓝色LED, 2的8次方个全彩色LED, 胸前按钮和双足各有1个RGB全彩色LED。 使用的坐标系：笛卡尔坐标系，X轴指向身体前方，Y为由右向左方向，Z轴为垂直向上方向。 沿Z轴方向的旋转称为偏转(Yam), 沿Y轴的旋转称为俯仰（Pitch)，沿X轴方向的旋转称为横滚（Roll）。 关节运动范围:头部两个自由度分别控制Nao脑袋的扭转，其中控制头部关节在Z轴扭转的范围为-120°到120°，在Y轴前后运动的范围为-39°到39°；左右手臂各有5个自由度且呈对称分布，肩关节控制Y轴前后运动的范围为-120°到120°，控制Z轴左右运动的范围为0°到95°，肩关节控制在X轴扭转的范围为-90°到0°，肘关节控制在Z轴运动的范围为-120°到120°，腕关节控制在X轴扭转的范围为-105°到105°；Nao的左右手上各有一个自由度，其控制Nao的手部的打开或合拢；Nao左右腿的关节除髋关节和踝关节外其余均呈对称分布的，左髋关节控制腿部在Y轴前后运动的范围为-104.5°到28.5°，左髋关节控制腿部在X轴左右运动的范围为-25°到45°，左踝关节控制在Y轴前后运动的范围为-70.5°到54°，左控制X轴左右运动的范围为-45°到25°，右髋关节控制腿部在Y轴前后运动的范围为-104.5°到28.5°，右髋关节控制腿部在X轴左右运动的范围为-45°到25°，右踝关节控制在Y轴前后运动的范围为-70.5°到54°，右踝关节控制X轴左右运动的范围为-25°到45°；膝关节控制腿部在Y轴运动的范围为-5°到125°；髋部存在一个控制其髋部在Y轴运动的自由度，其运动范围为-65.62°到42.44°。 自由度： 机器人能够独立运动的关节数目称为机器人的运动自由度。头部有两个关节，可以做偏转(Yam)和俯仰（pitch），因此，头部的自由度为2.全身共有26个自由度。 转矩是一种力矩，力矩=力力臂 （N m)堵转转矩和标称转矩反应了电机在启动和政策工作状态下驱动力的大小。堵转转矩是指当电机转速为0时的转矩，如膝关节电机在启动或维持半蹲状态都处于堵转状态。额定转矩是电机可以长期稳定运行的转矩。 主要方法：学习NAOqi API goToPosture(postureName, speed):转到预定义姿势。阻塞调用。getPosture():返回当前姿势名称，如果当前姿势不是预定义姿势，返回unknow。阻塞调用。getPostureList():返回预定义姿势列表。阻塞调用。applayPosture(postureName, speed)：将机器人关节设置为预定义姿势对应的状态（没有中间动作）。阻塞调用。stopMove():停止当前动作。 Nao行走控制主要的三种方式moveTo:使机器人移动到指定位置，阻塞调用。(1).moveTo(x, y, theta), 移动到指定位置。(2)moveTo(x, y, theta, MoveConfig), 按给定的步态参数移动到指定位置。moveConfig为自定义步态参数列表，列表中的内容为步态参数键值对。(3)moveTo(controlPoints)，沿控制点移动到指定位置，controlPoints为控制点列表。(4)moveTo(controlPoints, moveConfig), movetoConfig为自定义步态参数列表。 move:move方法是机器人按指定速度行走,非阻塞调用。 (1)move(x, y, theta), 按指定速度行走，x为绕X方向速度（m/s), theta为绕Z轴旋转角速度(rad/s), 负数表示顺时针转动。非阻塞调用方法，需要time.sleep()延时，延时时间除了行走过程时间外，应还包括机器人走过程的初始化阶段和终止阶段。(2)move(x, y, theata, moveConfig), 按给定的步态参数和指定速度行走。其中x为X方向，moveConfig为自定义步态参数列表可以分别设置左脚和右脚的步态参数。 moveToward:moveToward()方法是机器人按指定速度行走，非阻塞调用方法。(1)moveToward(x, y, theta), 按指定速度行走，其中x为X方向速度，取值范围[-1, 1], theta为绕Z轴旋转速度。(2)moveToward(x, y, moveConfig), 按给定的步态参数、指定速度行走。","categories":[{"name":"NAO","slug":"NAO","permalink":"http://uscair.club/categories/NAO/"}],"tags":[{"name":"NAOqi","slug":"NAOqi","permalink":"http://uscair.club/tags/NAOqi/"}],"author":{"name":"HL"}},{"title":"STM32单片机系列(一)","slug":"STM32-CubMx","date":"2020-02-08T16:00:00.000Z","updated":"2020-02-08T16:00:00.000Z","comments":true,"path":"2020/02/09/STM32-CubMx/","link":"","permalink":"http://uscair.club/2020/02/09/STM32-CubMx/","excerpt":"","text":"介绍STM32CubeMX是一个图形化的工具，也是配置和初始化C代码生成器（STM32 configuration and initialization C code generation），也就是自动生成开发初期关于芯片相关的一些初始化代码。它包含了STM32所有系列的芯片，包含示例和样本（Examples and demos）、中间组件（Middleware Components）、硬件抽象层（Hardwaree abstraction layer）。CubMX对一些开发工程的基本配置提供了很大的帮助，大大减少了编写基础代码所耗费的时间以及精力。内容今天就以点亮LED灯为例通过CubMX来编写相关程序。首先选择对应的芯片类型这里以STM32F407IE为例：其次可以通过对应芯片的用户手册来确定对应的IO口：407对应的是F9,F10俩个IO口（任选其一即可）。)然后就是最重要的时钟配置，时钟是心脏是重中之重。设置了对应时钟源之后可以看到对应的IO口也有所改变 F407 LED的IO挂载在了APB1时钟线上，配置对应的时钟频率 其次就是代码生成的一些配置：选择工程的名字，以及生成的工程文件的类型，这我这里由于我选择的是Keil5所以我选择了MDK-ARM V5然后记得勾选生成外设源文件和头文件的选项最后点击生成代码","categories":[{"name":"单片机","slug":"单片机","permalink":"http://uscair.club/categories/%E5%8D%95%E7%89%87%E6%9C%BA/"}],"tags":[{"name":"STM32","slug":"STM32","permalink":"http://uscair.club/tags/STM32/"}],"author":{"name":"Page"}},{"title":"线性回归的几种解法","slug":"机器学习-线性回归","date":"2020-02-08T16:00:00.000Z","updated":"2020-02-08T16:00:00.000Z","comments":true,"path":"2020/02/09/机器学习-线性回归/","link":"","permalink":"http://uscair.club/2020/02/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","excerpt":"线性回归的几种简单解法总结","text":"线性回归的几种简单解法总结 最小二乘 和 正规方程假设线性回归方程为 $Y = W_0+W_1X1+W_2X_2+….$ 则最小二乘的条件是 $S =\\sum_1^n(y_i-(w_0+w_1x_{1i}+…))^2 = min$ 且 $$\\frac{\\sigma S}{\\sigma x_i} = 0$$ 令$X=\\begin{bmatrix}1&amp;x_1^1&amp;\\cdots&amp;x_n^1\\1&amp;x_1^2&amp;\\cdots&amp;x_n^2\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\1&amp;x_1^n&amp;\\cdots&amp;x_n^n\\\\end{bmatrix}$ $Y =\\begin{bmatrix}y_1\\y_2\\\\vdots\\y_n\\end{bmatrix}$ $W=[W_0,W_1,W_2,W_3 ,….]$ 则 $Y=XW^T$ $\\sum_1^n(y_i-(w_0+w_1x_{1i}+…))^2$ $=(XW^T-Y)^T(XW^T-Y)$ $=(WX^T-Y^T)(XW^T-Y)$ $=WX^TXW^T-Y^TXW^T-WX^TY+Y^TY$ 对W求导，可得 $2WX^TX-2Y^TX=0$ $W=(Y^TX)(X^TX)^{-1}$ $W^T=(XX^T)^{-1}X^TY$ 123456# 1 正规方程解法# W = (XTX)-1XTYdef Fun(X, Y): XTY = np.dot(X.T, Y) XTX = np.dot(X.T, X) return np.dot(np.linalg.inv(XTX), XTY) 梯度下降假设函数 $h(X)=w_0+w_1x_1+ …$ 代价函数 $J(W)=\\frac{1}{2n}\\sum_1^n(h(x^i)-y_i)^2$ 梯度下降算法 $w_i=w_i-\\alpha \\frac{J(W)}{\\alpha w_i}$ 其中$\\alpha$为学习率 即为 $w_i = w_i - \\alpha \\frac{1}{n}(h(x^i)-y_i)x_i$ 样例如下: 12345678910111213141516171819202122232425import numpy as np# 数据生成num = 8num_examples = 1000 # 样例数# 真实参数 f(x1,x2,x2,x4) = w0+w1x3+w2x3+w3x3+w4x4true_w = np.random.randint(-10000, 10000, num)X = np.random.randint(-10000, 10000, (num_examples, num))X[:, 0] = 1# Y= XWY = np.dot(X, true_w)# 预置a = 0.000000001W = np.zeros(num)def BP(X, Y): for i in range(X.shape[0]): h = np.dot(X[i], W) for j in range(W.shape[0]): W[j] = W[j] - a *(h-Y[i]) * X[i][j] /num_examplesfor i in range(1000): BP(X,Y)print(\"原: \",true_w )print(\"解: \", W)","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"线性回归","slug":"线性回归","permalink":"http://uscair.club/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"}],"author":{"name":"RE9T"}},{"title":"Robocup新手指南","slug":"Robocup新手指南","date":"2020-02-07T16:00:00.000Z","updated":"2020-02-07T16:00:00.000Z","comments":true,"path":"2020/02/08/Robocup新手指南/","link":"","permalink":"http://uscair.club/2020/02/08/Robocup%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97/","excerpt":"","text":"初学者该从哪些方面了解1.robocup比赛过程以及相关规则 2.什么是robocup robocup简介RoboCup (Robot World Cup)，即机器人世界杯足球锦标赛。它是国际上一项为提高相关领域的教育和研究水平而举行的大型比赛和学术活动,通过提供一个标准任务来促进分布式人工智能、智能机器人技术、及其相关领域的研究与发展。 server能在server上工作是建立在对SPADES上的，所以接下来主要介绍一下SPADES相关的两点主要内容，即Agent可接收的感觉和动作 感觉感觉信息格式：Stime time data其中第一个time指的是感觉发出的周期，其二个time指的是信息到达agent的周期，data指感觉信息的字符串，值得注意的是一条感觉信息可以包含多种感觉。1.视觉：全方向，所感观物体是透明的，物体位置以相对于agent的极坐标形式给出，坐标参数为（距离，theta，phi）2.competition state即比赛状态：主要提供信息，例如球门球员尺寸,物体质量，时间以及比赛模式，球员号码，球员是左还是右的位置3.agent状态：主要提供自身内部信息，即电池状态和温度4.听觉：可通过听说来相互通讯 动作与感觉一样，通过一些动作来实现agent对环境的作用。每种动作信息以字母”A”开头，后面是字符串。格式：Adatadata包含动作信息Creat：这是agent必须发送的第一个动作。这个动作使得server登录agent并创建与agent的通讯。格式如下： A(create) 类似的还有Init，Beam，Drive，Kick，Catch，Say等，具体内容可点击下方链接查看。 关于3D的具体内容指导","categories":[{"name":"人工智能","slug":"人工智能","permalink":"http://uscair.club/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"robocup","slug":"robocup","permalink":"http://uscair.club/tags/robocup/"}],"author":{"name":"徐雯君"}},{"title":"数字图像处理——拉普拉斯算子【像素级别处理】（python）","slug":"数字图像处理——拉普拉斯算子【像素级别处理】（python）","date":"2020-02-06T12:34:46.135Z","updated":"2020-02-06T12:34:46.135Z","comments":true,"path":"2020/02/06/数字图像处理——拉普拉斯算子【像素级别处理】（python）/","link":"","permalink":"http://uscair.club/2020/02/06/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E2%80%94%E2%80%94%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E7%AE%97%E5%AD%90%E3%80%90%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%88%AB%E5%A4%84%E7%90%86%E3%80%91%EF%BC%88python%EF%BC%89/","excerpt":"","text":"简介：==拉普拉斯算子是一种微分算子常在图像处理中强调灰度值的突变，不强调灰度变换缓慢的地方，得到的图层与原图像叠加在一起可以得到锐化的效果== 一个二维图像的拉普拉斯算子可以定义为$$\\nabla^{2}f=\\frac{\\partial^2 f}{\\partial x^2}+\\frac{\\partial^2 f}{\\partial y^2}$$ 所以:在X方向上存在$$\\frac{\\partial^2 f}{\\partial x}=f(x+1,y)+f(x-1,y)-2f(x,y)$$ 在Y方向上存在$$\\frac{\\partial^2 f}{\\partial y}=f(x,y+1)+f(x,y-1)-2f(x,y)$$ 可得：$$\\nabla^2f=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)-4f(x,y)$$ 扩展至对角线：$$\\nabla^2f=f(x+1,y)+f(x-1,y)+f(x,y+1)+f(x,y-1)+f(x-1,y-1)+f(x-1,y+1)+f(x+1,y-1)+f(x+1,y+1)-8f(x,y)$$ 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186import cv2import numpy as npimport matplotlib.pyplot as pltimg = cv2.imread('Fig0338.tif') # 测试图片H = img.shape[0]W = img.shape[1]pixa = np.zeros((H, W), np.int32)mImgae = np.zeros((H, W, 3), np.uint8) # 标定(scale)前的滤波图像smImga = np.zeros((H, W, 3), np.uint8) # 标定(scale)后的滤波图像pixb = np.zeros((H, W), np.int32)mImgbe = np.zeros((H, W, 3), np.uint8) # 标定前的滤波图像smImgb = np.zeros((H, W, 3), np.uint8) # 标定后的滤波图像imga = np.zeros((H, W, 3), np.uint8) # xy方向模板滤波后图像imgb = np.zeros((H, W, 3), np.uint8) # 加上对角方向模板滤波后图像# a用到的算子是 b用到的算子是# 0 1 0 1 1 1# 1 -4 1 1 -8 1# 0 1 0 1 1 1# 先绘制标定滤波图像# 标定指的是最小值设置为0，最大值设置为255的进行归一化的结果for i in range(1, H - 1): for j in range(1, W - 1): pixa[i, j] = int(img[i - 1, j, 0]) + img[i + 1, j, 0] + img[i, j - 1, 0] + img[i, j + 1, 0] - 4 * int( img[i, j, 0]) pixb[i, j] = int(img[i - 1, j - 1, 0]) + img[i - 1, j, 0] + img[i - 1, j + 1, 0] + img[i, j - 1, 0] + img[ i, j + 1, 0] + img[i + 1, j - 1, 0] + img[i + 1, j, 0] + img[i + 1, j + 1, 0] - 8 * int(img[i, j, 0])maxa = 0maxb = 0mina = 255minb = 255for i in range(H): for j in range(W): # 求出像素最大值和最小值，以利于scale if pixa[i, j] &gt; maxa: maxa = pixa[i, j] if pixa[i, j] &lt; mina: mina = pixa[i, j] if pixb[i, j] &gt; maxb: maxb = pixb[i, j] if pixb[i, j] &lt; minb: minb = pixb[i, j] if pixa[i, j] &lt; 0: mImgae[i, j] = [0, 0, 0] else: mImgae[i, j, 0] = pixa[i, j] mImgae[i, j, 1] = pixa[i, j] mImgae[i, j, 2] = pixa[i, j] if pixb[i, j] &lt; 0: mImgbe[i, j] = [0, 0, 0] else: mImgbe[i, j, 0] = pixb[i, j] mImgbe[i, j, 1] = pixb[i, j] mImgbe[i, j, 2] = pixb[i, j]ka = 0kb = 0if maxa &gt; mina: ka = 255 / (maxa - mina)if maxb &gt; minb: kb = 255 / (maxb - minb)# scale处理for i in range(H): for j in range(W): smImga[i, j, 0] = (pixa[i, j] - mina) * ka smImga[i, j, 1] = smImga[i, j, 0] smImga[i, j, 2] = smImga[i, j, 0] smImgb[i, j, 0] = (pixb[i, j] - minb) * kb smImgb[i, j, 1] = smImgb[i, j, 0] smImgb[i, j, 2] = smImgb[i, j, 0]# 加上拉普拉斯算子# pixa和pixb里面就是两个算子的结果# lapa和lapb是原图加算子的结果，用来裁剪或者scale的原始数据lapa = np.zeros((H, W), np.int32)lapb = np.zeros((H, W), np.int32)# 缩放处理# maxa = 0# maxb = 0# mina = 255# minb = 255for i in range(H): for j in range(W): lapa[i, j] = img[i, j, 0] - pixa[i, j] lapb[i, j] = img[i, j, 0] - pixb[i, j] # 裁剪处理 if lapa[i, j] &gt; 255: lapa[i, j] = 255 if lapa[i, j] &lt; 0: lapa[i, j] = 0 if lapb[i, j] &gt; 255: lapb[i, j] = 255 if lapb[i, j] &lt; 0: lapb[i, j] = 0 # 缩放处理 # if lapa[i, j] &gt; maxa: # maxa = lapa[i, j] # if lapa[i, j] &lt; mina: # mina = lapa[i, j] # if lapb[i, j] &gt; maxb: # maxb = lapb[i, j] # if lapb[i, j] &lt; minb: # minb = lapb[i, j]# 缩放处理# ka = 0# kb = 0# if maxa &gt; mina:# ka = 255 / maxa# if maxb &gt; minb:# kb = 255 / maxb# scale处理for i in range(H): for j in range(W): # 裁剪处理 imga[i, j, 0] = lapa[i, j] imga[i, j, 1] = lapa[i, j] imga[i, j, 2] = lapa[i, j] imgb[i, j, 0] = lapb[i, j] imgb[i, j, 1] = lapb[i, j] imgb[i, j, 2] = lapb[i, j] # 缩放处理 # if lapa[i, j] &gt; 0: # imga[i, j, 0] = lapa[i, j] * ka # else: # imga[i, j, 0] = 0 # imga[i, j, 1] = imga[i, j, 0] # imga[i, j, 2] = imga[i, j, 0] # if lapb[i, j] &gt; 0: # imgb[i, j, 0] = lapb[i, j] * kb # else: # imgb[i, j, 0] = 0 # imgb[i, j, 1] = imgb[i, j, 0] # imgb[i, j, 2] = imgb[i, j, 0]# 原图plt.subplot(1, 4, 1)plt.axis('off')plt.title('Original image')plt.imshow(img)# 图3.37a的模板plt.subplot(2, 4, 2)plt.axis('off')plt.title('Before sale a')plt.imshow(mImgae)# scale后图3.37a的模板plt.subplot(2, 4, 3)plt.axis('off')plt.title('After sale a')plt.imshow(smImga)# 图3.37a的模板锐化后的图像plt.subplot(2, 4, 4)plt.axis('off')plt.title('Sharpened Image a')plt.imshow(imga)# 图3.37b的模板plt.subplot(2, 4, 6)plt.axis('off')plt.title('Before sale b')plt.imshow(mImgbe)# scale后图3.37b的模板plt.subplot(2, 4, 7)plt.axis('off')plt.title('After sale b')plt.imshow(smImgb)# 图3.37b的模板锐化后的图像plt.subplot(2, 4, 8)plt.axis('off')plt.title('Sharpened Image b')plt.imshow(imgb)plt.show()","categories":[{"name":"数字图像处理","slug":"数字图像处理","permalink":"http://uscair.club/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"拉普拉斯","slug":"拉普拉斯","permalink":"http://uscair.club/tags/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF/"}],"author":{"name":"Gowi"}},{"title":"吴恩达机器学习第三章【Linear Algebra Revie】（线性代数回顾）","slug":"吴恩达机器学习第三章【Linear Algebra Revie】","date":"2020-02-06T04:58:30.386Z","updated":"2020-02-06T04:58:30.386Z","comments":true,"path":"2020/02/06/吴恩达机器学习第三章【Linear Algebra Revie】/","link":"","permalink":"http://uscair.club/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%90Linear%20Algebra%20Revie%E3%80%91/","excerpt":"","text":"Matrices and Vectors【矩阵和向量】在$\\begin{bmatrix}{1402}&amp;{191}\\\\{1371}&amp;{821}\\\\{949}&amp;{1437}\\\\{147}&amp;{1448}\\end{bmatrix}$中，这是一个这个是4×2矩阵，即4行2列，如$m$为行，$n$为列，那么$m×n$即4×2，其中$A_{ij}$指第$i$行，第$j$列的元素。 向量是一种特殊的矩阵，向量一般都是列向量，如：$y=\\left[ \\begin{matrix} {460} \\ {232} \\ {315} \\ {178} \\\\\\end{matrix} \\right]$为四维列向量（4×1）。 Addition and Scalar Multiplication【加法和标量乘法】矩阵的加法：行列数相等的各元素相加。 $\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}+\\begin{bmatrix}{4}&amp;{0.5}\\\\{2}&amp;{5}\\\\{0}&amp;{1}\\end{bmatrix}=\\begin{bmatrix}{5}&amp;{0.5}\\\\{4}&amp;{10}\\\\{3}&amp;{2}\\end{bmatrix}$ 矩阵的数乘：每个元素都要乘。 $3\\times\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}=\\begin{bmatrix}{3}&amp;{0}\\\\{6}&amp;{15}\\\\{9}&amp;{3}\\end{bmatrix}=\\begin{bmatrix}{1}&amp;{0}\\\\{2}&amp;{5}\\\\{3}&amp;{1}\\end{bmatrix}\\times3$ Matrix Vector Multiplication【矩阵向量乘法】 Matrix Matrix Multiplication【矩阵乘法】矩阵乘法： $m×n$矩阵乘以$n×o$矩阵，变成$m×o$矩阵。 在单变量线性回归中的应用 Matrix Multiplication Properties【矩阵乘法的性质】矩阵乘法的性质： 矩阵的乘法不满足交换律：$A×B≠B×A$ 矩阵的乘法满足结合律。即：$A×(B×C)=(A×B)×C$ 单位矩阵：在矩阵的乘法中，有一种矩阵起着特殊的作用，如同数的乘法中的1,我们称这种矩阵为单位矩阵．它是个方阵，一般用 $I$ 或者 $E$ 表示,从左上角到右下角的对角线（称为主对角线）上的元素均为1以外全都为0 $A{{A}^{-1}}={{A}^{-1}}A=I$ 对于单位矩阵，有$AI=IA=A$ Inverse and Transpose【逆、转置】矩阵的逆：如矩阵$A$是一个$m×m$矩阵（方阵），如果有逆矩阵，则：$A{{A}^{-1}}={{A}^{-1}}A=I$ 矩阵的转置：设$A$为$m×n$阶矩阵（即$m$行$n$列），第$i $行$j $列的元素是$a(i,j)$，即：$A=a(i,j)$ 定义$A$的转置为这样一个$n×m$阶矩阵$B$，满足$B=a(j,i)$，即 $b (i,j)=a(j,i)$（$B$的第$i$行第$j$列元素是$A$的第$j$行第$i$列元素），记${{A}^{T}}=B$。(有些书记为A’=B） ${{\\begin{bmatrix} a& b \\\\ c& d \\\\ e& f \\\\\\end{bmatrix} }^{T}}=\\begin{bmatrix} a&amp; c &amp; e \\ b&amp; d &amp; f \\\\\\end{bmatrix}$ 矩阵的转置基本性质: $ {{\\left( A\\pm B \\right)}^{T}}={{A}^{T}}\\pm {{B}^{T}} $${{\\left( A\\times B \\right)}^{T}}={{B}^{T}}\\times {{A}^{T}}$${{\\left( {{A}^{T}} \\right)}^{T}}=A $${{\\left( KA \\right)}^{T}}=K{{A}^{T}} $","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"http://uscair.club/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"},{"name":"数理基础","slug":"数理基础","permalink":"http://uscair.club/tags/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80/"}],"author":{"name":"Gowi"}},{"title":"吴恩达机器学习第二章【Linear Regression with One Variable】（单变量线性回归）","slug":"Linear Regression with One Variable","date":"2020-02-06T04:58:25.879Z","updated":"2020-02-06T04:58:25.879Z","comments":true,"path":"2020/02/06/Linear Regression with One Variable/","link":"","permalink":"http://uscair.club/2020/02/06/Linear%20Regression%20with%20One%20Variable/","excerpt":"","text":"##Model Representation【模型表示】 在一个房价预测中，我们根据房屋大小的面积来估计房价，诺房屋面积与价格满足以下关系 由图可知为监督学习，诺一种可能的表达方式为：$h_\\theta \\left( x \\right)=\\theta_{0} + \\theta_{1}x$，因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归(Linear Regression with One Variable)问题。 图为用单变量线性回归模型来预测面积为1250的房价 我们将要用来描述这个回归问题的标记如下: $m$ 代表训练集中实例的数量 $x$ 代表特征/输入变量 $y$ 代表目标变量/输出变量 $\\left( x,y \\right)$ 代表训练集中的实例 $({{x}^{(i)}},{{y}^{(i)}})$ 代表第$i$ 个观察实例 $h$ 代表学习算法的解决方案或函数也称为假设（hypothesis） Cost Function【代价函数】我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的差距（下图中蓝线所指）就是建模误差（modeling error）。 在单变量线性回归中，即使得代价函数(Cost Function) $J \\left( \\theta_0, \\theta_1 \\right) = \\frac{1}{2m}\\sum\\limits_{i=1}^m \\left( h_{\\theta}(x^{(i)})-y^{(i)} \\right)^{2}$最小。 Cost Function - Intuition I【代价函数的直观理解I】 在单变量线性回归中，存在点$(1,1),(2,2),(3,3)$，在$\\theta_0=0$的情况下，表达式为$h(x)=\\theta_1x$来估计 $\\theta_1=1$时： $\\theta_1=\\frac{1}{2}$时： $\\theta_1=0$时： 描绘$J(\\theta_1)-\\theta_1$图得： Cost Function - Intuition II【代价函数的直观理解II】在$J \\left( \\theta_0, \\theta_1 \\right)$、$\\theta_0$与$\\theta_1$的三维图中 可知存在使得$J(\\theta_0,\\theta_1)$最小的$\\theta_0和\\theta_1$ 利用等高图表示 Gradient Descent【梯度下降】梯度下降(Gradient Descent)背后的思想是：开始时我们随机选择一个参数的组合$\\left( {\\theta_{0}},{\\theta_{1}},……,{\\theta_{n}} \\right)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到找到一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。 即$Min\\sum_{\\theta_1、\\theta_2、\\theta_3···\\theta_n}J(\\theta_1,\\theta_2,\\theta_3···\\theta_n)$ 批量梯度下降（batch gradient descent）算法的公式为： 其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。其中:=相当于计算机中的赋值号。 Gradient Descent Intuition【梯度下降的直观理解】梯度下降算法如下： ${\\theta_{j}}:={\\theta_{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)$ 描述：对$\\theta $赋值，使得$J\\left( \\theta \\right)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中$a$是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大 其中$\\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)$表示切线的斜率 让我们来看看如果$a$太小或$a$太大会出现什么情况： 如果$a$太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果$a$太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。 如图： 如果$a$太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果$a$太大，它会导致无法收敛，甚至发散。 如图： 假设你将${\\theta_{1}}$初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得${\\theta_{1}}$不再改变，也就是新的${\\theta_{1}}$等于原来的${\\theta_{1}}$，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率$a$保持不变时，梯度下降也可以收敛到局部最低点。 首先初始化我的梯度下降算法，在那个品红色的点初始化，如果我更新一步梯度下降，也许它会带我到这个点，因为这个点的导数是相当陡的。现在，在这个绿色的点，如果我再更新一步，你会发现我的导数，也即斜率，是没那么陡的。随着我接近最低点，我的导数越来越接近零，所以，梯度下降一步后，新的导数会变小一点点。然后我想再梯度下降一步，在这个绿点，我自然会用一个稍微跟刚才在那个品红点时比，再小一点的一步，到了新的红色点，更接近全局最低点了，因此这点的导数会比在绿点时更小。所以，我再进行一步梯度下降时，我的导数项是更小的，${\\theta_{1}}$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，已经收敛到局部极小值。在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$a$。 Gradient Descent For Linear Regression【梯度下降的线性回归】 $\\frac{\\partial }{\\partial {{\\theta }_{j}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{\\partial }{\\partial {{\\theta }_{j}}}\\frac{1}{2m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}^{2}}$ $j=0$ 时：$\\frac{\\partial }{\\partial {{\\theta }_{0}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}}$ $j=1$ 时：$\\frac{\\partial }{\\partial {{\\theta }_{1}}}J({{\\theta }_{0}},{{\\theta }_{1}})=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}$ 则算法改写成： Repeat { ​ ${\\theta_{0}}:={\\theta_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{ \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}$ ​ ${\\theta_{1}}:={\\theta_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}$ ​ } 在梯度下降中会一步步逼近代价函数最小的值","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"http://uscair.club/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"author":{"name":"Gowi"}},{"title":"吴恩达机器学习第一章【Introduction】","slug":"Introduction","date":"2020-02-06T04:58:21.048Z","updated":"2020-02-06T04:58:21.048Z","comments":true,"path":"2020/02/06/Introduction/","link":"","permalink":"http://uscair.club/2020/02/06/Introduction/","excerpt":"","text":"Machine Learning机器学习所研究的主要内容，是关于计算机上从数据中产生“模型”（model）的算法，即“学习算法”（learning algorithm），有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型；面对新的情况时，模型会给我们提供相应的判断。其中从数据中学得模型的过程称为称为“训练”（training）或“学习”（learning） 123graph LR机器学习--有标记信息--&gt;监督学习机器学习--无标记信息--&gt;无监督学习 机器学习的目标是使学得的模型能很好的适应于“新样本”，而不是在新样本上工作的好好的，学得的模型适用于新样本的能力，称为“泛化”能力。 Supervised Learning【监督学习】定义：根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。也就是说，在监督学习中训练数据==既有特征(feature)又有标签(label)==，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。 监督学习的分类：回归(Regression）、分类（Classification) 回归问题：针对连续型问题 回归通俗一点就是，对已经存在的点（训练数据）进行分析，拟合出适当的函数模型y=f(x)，这里y就是数据的标签，而对于一个新的自变量x，通过这个函数模型得到标签y。 分类问题：针对离散型问题 Unsupervised Learning【无监督学习】定义：我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。 可以这么说，比起监督学习，无监督学习更像是自学，让机器学会自己做事情，是没有标签（label）的。我们只是给定了一组数据，我们的目标是发现这组数据中的特殊结构。例如我们使用无监督学习算法会将这组数据分成两个不同的簇,，这样的算法就叫聚类算法。 Answer : B、C","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"机器学习基础","slug":"机器学习基础","permalink":"http://uscair.club/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"}],"author":{"name":"Gowi"}},{"title":"博客模板","slug":"博客模板","date":"2020-02-05T16:00:00.000Z","updated":"2020-02-05T16:00:00.000Z","comments":true,"path":"2020/02/06/博客模板/","link":"","permalink":"http://uscair.club/2020/02/06/%E5%8D%9A%E5%AE%A2%E6%A8%A1%E6%9D%BF/","excerpt":"模板格式","text":"模板格式 -— title: 文章标题 date: 2019-08-18 author: &nbsp;&nbsp;name: 作者名 categories: &nbsp;&nbsp;-&nbsp;数字图像处理 #分类 tags: &nbsp;&nbsp;-&nbsp;OpenCV #标签 music: &nbsp;&nbsp;enable: true # true（文章内和文章列表都显示） internal（只在文章内显示） &nbsp;&nbsp;server: netease # netease（网易云音乐）tencent（QQ音乐） xiami（虾米） kugou（酷狗） &nbsp;&nbsp;type: song # song （单曲） album （专辑） playlist （歌单） search （搜索） &nbsp;&nbsp;id: 1321385758 # 歌曲/专辑/歌单 ID mathjax: true #博客内有公式要加上 -— 摘要 &lt;!-- more --&gt; (上为摘要，下为正文) 正文 将需要放大预览的图片用 &lt;fancybox&gt; &lt;\\fancybox&gt; 包含起来。 音乐id获取方法：eg: 网页版网易云搜索歌曲 如图（可能出现：“由于版权问题无法生成外链，建议换一首没有版权限制的”） markdown编辑器：typora 关于markdown加载图片：推荐使用图床(https://zhuanlan.zhihu.com/p/35270383?ivk_sa=1023345p) 填入图床连接 关于markdown数学公式可以使用mathjax(https://www.jianshu.com/p/a0aa94ef8ab2)","categories":[{"name":"Hexo","slug":"Hexo","permalink":"http://uscair.club/categories/Hexo/"}],"tags":[{"name":"模板","slug":"模板","permalink":"http://uscair.club/tags/%E6%A8%A1%E6%9D%BF/"}]},{"title":"支持向量机(SVM)","slug":"支持向量机-SVM","date":"2020-01-23T08:16:00.000Z","updated":"2020-01-23T08:16:00.000Z","comments":true,"path":"2020/01/23/支持向量机-SVM/","link":"","permalink":"http://uscair.club/2020/01/23/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/","excerpt":"2020/01/23","text":"2020/01/23 支持向量机（Support vector machines）一、简介支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，分割的原则是间隔最大化，最终转化为一个凸二次规划问题来求解。由简至繁的模型包括： 当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机； 当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机； 当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机； 与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。 二、SVM 与 Logistic regression逻辑回归预测函数： 现在考虑下我们想要逻辑回归做什么：当 y=1 时， 我们需要预测函数 h(x)≈1, 需要 Z &gt;= 0当 y=0 时， 我们需要预测函数 h(x)≈0, 需要 Z &lt;= 0则对应的代价函数(z = theta.T * x) 当y值不同时，J(z)函数的曲线可大致拟合成一条折线cost(z)。将逻辑回归中J(z)替换为拟合的这条折线cost(z)，我们得到一个新的最小化函数: 加入正则项，再让其在逻辑回归代价函数上面稍加变形，则得到了SVM的函数: 最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数theta时，支持向量机所做的是它来直接预测y的值等于1，还是等于0。 三、SVM人们有时将支持向量机看作是大间距分类器。在训练得到最小化的theta过程中，先忽略参数C和正则项,则theta的变化方向是使得 当 y=1 时, z&gt;= 1 ，当 y=0 时, z&lt;=-1 （而不是逻辑回归中的0) 如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。 这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距(margin)。 当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。 参数C对划分的影响: C较大时，可能会导致过拟合,高方差 C较小时，可能会导致欠拟合,高偏差 例如：当 C较小时会得到黑线，而增大C使得C非常大时，会得到粉色线 关于SVM如何做到大间距分类这里先不解释。 四、核函数许多时候，我们面临的分类问题并不只是线性分类，还会遇到很多无法通过直线进行分隔的分类情况： 我们可以用一系列的新的特征f来替换模型中的每一项。将f代替x来对进行分类。而这个代替的变化函数则叫做核函数。例如： 这是一个高斯核函数(Gaussian Kernel)。其中l为landmark，为数据中的地标，使用数据与地标的距离大小计算用来取新的特征，距离越大获得的f值则越小，当距离为0时，f达到最大为1。例如在坐标中取3个地标: 这样得到的新特征与地标相关系，关联到数据与地标间的欧式距离，而训练出的theta也会通过这个核函数得到复杂的拟合边界。 其中高斯核函数中的参数 sigma 为用户定义的到达率，为核函数跌至0的速率参数 一般实现核函数SVM时使用的地标为整个数据集，即如果训练集有m个样本，则选取m个地标。 两个参数 C 和 sigma 的影响 C较大时，可能会导致过拟合,高方差 C较小时，可能会导致欠拟合,高偏差 sigma 较大时，可能导致低方差，高偏差 （欠拟合） sigma 较小时，可能导致低偏差，高方差 （过拟合） 除了高斯核函数外，还有很多其他的核函数，这里不一一介绍了。 这里只是初步对 SVM 作介绍和一些使用方法，其数学原理将在后续补充。 了解更多人工智能方面欢迎光顾个人博客： 戴挽舟的博客","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://uscair.club/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"AI大数据","slug":"AI大数据","permalink":"http://uscair.club/tags/AI%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"author":{"name":"戴挽舟（BbiHH）","url":"https://戴挽舟.fun"}},{"title":"Hello World","slug":"hello-world","date":"2020-01-13T08:55:07.598Z","updated":"2020-01-13T08:55:07.598Z","comments":true,"path":"2020/01/13/hello-world/","link":"","permalink":"http://uscair.club/2020/01/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"test","slug":"test","permalink":"http://uscair.club/categories/test/"}],"tags":[{"name":"test","slug":"test","permalink":"http://uscair.club/tags/test/"}]},{"title":"NAO机器人的小记","slug":"NAO机器人的小记","date":"2019-11-09T16:00:00.000Z","updated":"2019-11-09T16:00:00.000Z","comments":true,"path":"2019/11/10/NAO机器人的小记/","link":"","permalink":"http://uscair.club/2019/11/10/NAO%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E5%B0%8F%E8%AE%B0/","excerpt":"","text":"joint 摄像机相关参数 APIALAutonomousLife孤立状态solitary任何活动都可以通过ALAutonomousLifeProxy :: switchFocus（）启动。启动交互式Activity将切换到交互状态。在保障和禁用状态可以进入。无法停止自动启动板，其建议的活动将自动聚焦，可能会中断正在运行的活动。 交互状态interactive可以通过ALAutonomousLifeProxy :: switchFocus（）启动交互式活动聚焦新的交互式Activity将重新进入交互状态。孤立活动可能无法启动，这将导致孤立状态强制执行其规则并与用户创建不良生命周期。（这可能在未来发生变化）在保障和禁用状态可入以被输。自动启动板可以启动，它不会强制关注其建议。它不会中断正在运行的Activity。当整个交互活动堆栈退出时，将进入单独状态。当所有人都失去时，互动活动将被强行退出。（这尚未实施） 禁用状态disabled没有Activity可以通过ALAutonomousLifeProxy :: switchFocus（）启动无法启动自动启动板。该保障或交互状态可能无法进入。（可调用ALAutonomousLifeProxy :: setState（）退出此状态） 保护状态safeguard没有Activity可以通过ALAutonomousLifeProxy :: switchFocus（）启动 停止聚焦活动并清除堆栈。基本意识和呼吸服务如果正在运行则会停止。自动启动板已停止。然后处理反射。根据反射的结果，可以重新进入孤立状态。（对于损坏的硬件，机器人不会退出保护状态。对于过热，机器人在充分冷却后退出保护状态。） ALRobotPostureALRobotPostureProxy :: getPostureList std :: vector ALRobotPostureProxy :: getPostureList （） 返回：所有预定义姿势的矢量（名称） ALRobotPostureProxy :: getPosture std :: string ALRobotPostureProxy :: getPosture （） 返回当前预定义姿势的名称。如果当前姿势不在预定义姿势中，则返回“未知”。 ALRobotPostureProxy :: goToPosture bool ALRobotPostureProxy :: goToPosture （const std :: string postureName，const float speed ） 使机器人进入参数中要求的预定义姿势。可以修改移动的速度。移动是“智能的”：它将从机器人的开始姿势开始，并选择所有步骤以达到要求的姿势。这是一个阻塞调用。 ALMotionALMotionProxy :: setStiffnesses void ALMotionProxy :: setStiffnesses （const AL :: ALValue＆ names，const AL :: ALValue＆ stiffnesses ）设置一个或多个关节的刚度。这是一个非阻塞调用。names - 关节，链条，“主体”，“JointActuators”，“接头”或“执行器”的名称。stiffnesses - 零和一之间的一个或多个刚度。 示例：＃显示如何将刚度设置为1.0 。 ＃注意，这样做可能很危险，使用 ＃stiffnessInterpolation方法更安全，该方法需要持续时间参数。 names =’Body’ ＃如果只接收到一个参数，这将应用于所有关节 stiffnesses = 1.0 motionProxy.setStiffnesses(names, stiffnesses) ALMotionProxy :: getStiffnesses) std::vectorALMotionProxy::getStiffnesses(const AL::ALValue&amp; jointName)获取关节或关节组的刚度jointName - 关节，链，“Body”，“JointActuators”，“Joints”或“Actuators”的名称。返回：一个或多个刚度。1.0表示最大刚度。0.0表示最小刚度 ALMotionProxy :: angleInterpolation void ALMotionProxy :: angleInterpolation （const AL :: ALValue＆ names，const AL :: ALValue＆ angleLists，const AL :: ALValue＆ timeLists，const bool＆isAbsolute ）将一个或多个关节插值到目标角度或沿着定时轨迹。这是一个阻塞调用。names– 关节，链条，“主体”，“JointActuators”，“接头”或“执行器”的名称或名称。angleLists - 以弧度表示的角度，角度列表或角度列表列表timeLists - 时间，时间列表或时间列表列表，以秒为单位isAbsolute - 如果为true，则以绝对角度描述运动，否则角度相对于当前角度。 示例：＃显示一个关节的单个目标角度的示例 ＃在1.0秒内将头部偏航插入1.0弧度 names =“HeadYaw” angleLists = 50.0 * almath.TO_RAD timeLists = 1.0 isAbsolute = True motionProxy.angleInterpolation（names，angleLists，timeLists，isAbsolute） ＃显示一个关节的单个轨迹的示例 ＃将头部偏航插入1.0弧度，并在2.0秒内回零 names =“HeadYaw” ＃2角度 angleLists = [30.0 * almath.TO_RAD，0.0] ＃ 2次 timeLists = [1.0,2.0] #后面的时间一定要比原来的大（timeLists = [1.0,1.0] 报错：times must be increasing 所以应该表示的是总的时间 在1s内转30度 在2s内转30度再转回来 即转回的时间也是1s） isAbsolute = True motionProxy.angleInterpolation（names，angleLists，timeLists，isAbsolute） ＃显示多个轨迹的示例 names = [“HeadYaw”，“HeadPitch”] angleLists = [30.0 almath.TO_RAD，30.0 almath.TO_RAD] #左下方 timeLists = [1.0,1.2] #（可以为timeLists = [1.0,1.0]） isAbsolute = True motionProxy.angleInterpolation（names，angleLists，timeLists，isAbsolute） ＃显示多个轨迹的示例 ＃将头部偏航插入1.0弧度，并在2.0秒内回零 ＃在长时间内上下插入HeadPitch。 names = [“HeadYaw”，“HeadPitch”] ＃每个关节可以有不同长度的列表，但数量 ＃角度和每个关节的次数必须相同。 ＃这里，第二个关节（“HeadPitch”）有三个角度，和 ＃三个相应的时间。 angleLists = [[50.0 almath.TO_RAD，0.0]， [-30.0 almath.TO_RAD，30.0 * almath.TO_RAD，0.0]] timeLists = [[1.0,2.0]，[1.0,2.0,3.0]] isAbsolute = True motionProxy.angleInterpolation（names，angleLists，timeLists，isAbsolute） ALMotionProxy :: angleInterpolationWithSpeed) void ALMotionProxy :: angleInterpolationWithSpeed （const AL :: ALValue＆ names，const AL :: ALValue＆targetAngles，const float＆maxSpeedFraction ）使用最大速度的一小部分将一个或多个关节插值到目标角度。每个关节只允许一个目标角度。这是一个阻塞调用。names - 关节，链条，“主体”，“JointActuators”，“接头”或“执行器”的名称或名称。targetAngles - 以弧度表示的角度或角度列表maxSpeedFraction - 一个分数 示例：＃显示一个关节的单个目标的示例 names =“HeadYaw” targetAngles = 1.0 maxSpeedFraction = 0.2＃使用最大关节速度的20％ motionProxy.angleInterpolationWithSpeed（names，targetAngles，maxSpeedFraction） ＃显示多个关节的示例 ＃而不是列出每个关节，您可以使用链名称 #bein扩展为包含链中的所有关节。在这种情况下， ＃“Head”将被解释为[“HeadYaw”，“HeadPitch”] names =“Head” ＃我们仍然需要指定正确的目标角度数 targetAngles = [0.5,0.25] maxSpeedFraction = 0.2＃使用最大关节速度的20％ motionProxy.angleInterpolationWithSpeed（names，targetAngles，maxSpeedFraction） ＃显示体零位置的示例 ＃而不是列出每个关节，您可以使用名称“Body” names =’Body’ ＃我们仍然需要指定正确的目标角度数，所以 ＃我们需要找到这个Nao的关节数量。 ＃这里我们使用getBodyNames方法，它告诉我们所有人 ＃别名“Body”中关节的名称。 ＃我们可以将此列表用于“names”参数。 numJoints = len(motionProxy.getBodyNames(“Body”)) ＃列出正确的长度。所有角度都为零。 targetAngles = [0.0] * numJoints ＃使用最大关节速度的10％ maxSpeedFraction = 0.1 motionProxy.angleInterpolationWithSpeed(names, targetAngles, maxSpeedFraction) ALMotionProxy :: move) void ALMotionProxy :: move （const float＆ x，const float＆ y，const float＆theta，const AL :: ALValue moveConfig ）1方法的重载使机器人以给定的速度移动，以FRAME_ROBOT表示，具有移动配置。这是一个非阻塞调用。x - 沿X轴的速度，以米/秒为单位。向后运动使用负值y - 沿Y轴的速度，以米/秒为单位。使用正值向左移动theta - 绕Z轴的速度，以弧度/秒为单位。使用负值顺时针旋转moveConfig - 具有自定义移动配置的ALValue。 ALMotionProxy :: moveTowardvoid ALMotionProxy :: moveToward （const float＆ x，const float＆ y，const float＆theta，const AL :: ALValue moveConfig ）使机器人以给定的标准化速度移动，以FRAME_ROBOT表示，具有移动配置。这是一个非阻塞调用。 x - 沿X轴标准化，无单位，速度。+1和-1分别对应于前向和后向的最大速度。y - 沿Y轴标准化，无单位，速度。+1和-1分别对应于左右方向上的最大速度。theta-标准化，无单位，绕Z轴的速度。+1和-1分别对应于逆时针和顺时针方向的最大速度moveConfig - 具有自定义移动配置的ALValue。 ＃示例显示moveToward的使用 ＃参数是最大值的分数 ＃这里我们要求全速前进 x = 1.0 y = 0.0 theta = 0.0 frequency = 1.0 motionProxy.moveToward(x, y, theta, [[“Frequency”, frequency]])＃如果我们不发送另一个命令，他将永远移动 ＃让我们让他慢下来（步长），并打开后3秒time.sleep(3) x = 0.5 theta = 0.6motionProxy.moveToward(x, y, theta, [[“Frequency”, frequency]]) ALMotionProxy :: setFootStepsWithSpeed) void ALMotionProxy :: setFootStepsWithSpeed （const std :: vector ＆ legName，const AL :: ALValue＆ footSteps，const std :: vector ＆ fractionMaxSpeed，const bool＆ clearExisting ）仅限NAO使机器人快速地做足步计划器。这是一个阻塞调用。legName - 要移动的腿的名称（’LLeg’or’RLeg’）footSteps - [x，y，theta]，[沿X / Y的位置，Z轴的方向]，相对于另一个腿，以[米，米，弧度]为单位。必须小于[MaxStepX，MaxStepY，MaxStepTheta]fractionMaxSpeed - 每步脚的速度。必须介于0和1之间clearExisting - 清除现有的脚步 ＃小步前进和逆时针用左脚 legName = [“LLeg”] X = 0.04 Y = 0.1 Theta = 0.3 footSteps = [[X, Y, Theta]] fractionMaxSpeed = [1.0] clearExisting = False motionProxy.setFootStepsWithSpeed(legName, footSteps, fractionMaxSpeed, clearExisting) ＃小步前进和逆时针用左脚 legName = [“LLeg”, “RLeg”] X = 0.04 Y = 0.1 Theta = 0.3 footSteps = [[X, Y, Theta], [X, -Y, Theta]] fractionMaxSpeed = [1.0, 1.0] clearExisting = FalsemotionProxy.setFootStepsWithSpeed(legName, footSteps, fractionMaxSpeed, clearExisting) ALMotionProxy :: waitUntilMoveIsFinished void ALMotionProxy :: waitUntilMoveIsFinished （） 等待MoveTask结束：此方法可用于阻止脚本/代码执行，直到完成移动任务。 ALMemoryALMemoryProxy :: getData ALMemoryProxy :: getDataList ALMemoryProxy :: declareEvent ALMemoryProxy :: insertData ALTrackerALTrackerProxy :: getActiveTarget ALTrackerProxy :: getMaximumDistanceDetection ALTrackerProxy::getMode ALTrackerProxy :: getRelativePosition ALTrackerProxy :: isNewTargetDetected ALTrackerProxy::isSearchEnabled ALTrackerProxy :: getTargetPosition ALTrackerProxy::isTargetLost ALTrackerProxy::setTimeOut ALVideoDeviceALVideoDeviceProxy :: getImagesRemote EventsALLandMarkDetection 结果变量的组织方式 ①如果未检测到Naomarks，则变量为空。更确切地说，它是一个零元素的数组（即在python中打印为[]） ②如果检测到N个Naomarks，则变量结构由两个字段组成：[[TimeStampField] [Mark_info_0，Mark_info_1 ,. 。。，Mark_info_N-1]]：A：TimeStampField = [TimeStamp_seconds，Timestamp_microseconds]。该字段是用于执行检测的图像的时间戳。B：Mark_info = [ShapeInfo，ExtraInfo]。对于每个检测到的标记，我们有一个Mark_info字段。B\\1 ShapeInfo = [1，alpha，beta，sizeX，sizeY，heading]。alpha和beta表示Naomark在摄像机角度方面的位置 - sizeX和sizeY是摄像机角度的标记大小 - 航向角描述了Naomark关于机器人头部的垂直轴方向。B\\2 ExtraInfo = [MarkID]。标记ID是写在Naomark上的编号，与其图案相对应。 ALRedBallDetection ALRedBallDetection基于摄像机给出的图像中红色像素的检测。这些像素根据它们与YUV颜色空间中的红色值的距离进行滤波，使用计算的阈值，即使在光照条件变化的情况下也可以进行检测。然后，从所有检测到的红色像素组中，仅保留定义圆形形状的红色像素。当在当前图像上找到一组像素时，将更新ALMemory键redBallDetected。 结果变量的组织形式 TimeStamp：此字段是用于执行检测的图像的时间戳 centerX和centerY是球的中心角度坐标（弧度）角度的原点是图像的中间。centerX 对应于沿Z轴的直接（逆时针）旋转，centerY对应于沿Y轴的直接旋转，如下图所示： sizeX和sizeY是球在角度（弧度）的球“水平和垂直半径 相关名词参考系 FRAME_TORSO：这是附加到机器人的躯干参考上的，因此机器人在走路时随其移动，在他倾斜时改变方向。当您执行非常局部的任务时（在躯干框架的方向上有意义），此空间很有用。 FRAME_ROBOT：这是围绕垂直Z轴投影的两只脚位置的平均值。该空间很有用，因为x轴始终向前，因此提供了一个以自我为中心的自然参考。 FRAME_WORLD：这是一个固定的原点，永远不会改变。当机器人行走时，它会被留下，并且在机器人转动之后z旋转会有所不同。此空间对于需要外部绝对参考框架的计算很有用。 执行任务时，空间是在任务开始时确定的，并且在其余的插值过程中始终保持不变。也就是说，插值一旦定义就不会随着腿的移动或躯干方向的变化而随着参考值的变化而变化。 刚度 阻塞方法与非阻塞方法 Position6DPosition6D是一个6维向量，由3个平移（以米为单位）和3个旋转（以弧度为单位）组成。 基于NAO机器人目标识别与定位算法 选自：柏雪峰,杨斌.基于NAO机器人目标识别与定位算法[J].成都信息工程学院学报,2014,29(06):625-629.","categories":[{"name":"NAO","slug":"NAO","permalink":"http://uscair.club/categories/NAO/"}],"tags":[{"name":"NAOqi","slug":"NAOqi","permalink":"http://uscair.club/tags/NAOqi/"}],"author":{"name":"Gowi"}},{"title":"OpenCV—Python:（一）图片的读取、显示、保存","slug":"opencv-read","date":"2019-08-18T09:34:16.000Z","updated":"2019-08-18T09:34:16.000Z","comments":true,"path":"2019/08/18/opencv-read/","link":"","permalink":"http://uscair.club/2019/08/18/opencv-read/","excerpt":"这篇文章描述了使用OpenCV进行图片的读取、显示、保存的方法。","text":"这篇文章描述了使用OpenCV进行图片的读取、显示、保存的方法。 图片的阅读、显示、保存：阅读：cv2.imread(“图片的路径”, 打开方式) 图片路径： 绝对路径：与代码不在同一目录里，写出详细地址，如“E:\\\\Photos\\\\aobing.jpg” 相对路径:代码相同目录中，只需写出图片名即可，如：“ao_bing.jpg” 注：不知道图片路径时，pycharm支持查看：点击图片文件名，单机单击鼠标右键， “Copy Path” 是绝对路径，”Copy Relative Path” 是相对路径! 打开方式： “ 1 “， 默认打开方式，加载彩色图像，任何图像的透明度都将忽略 “ 0 “， 加载灰度图像 “ -1 “，加载图像，包括alpha通道 显示：cv2.imshow(“窗口名称”，图片) 窗口名称：自己命名（不同的窗口使用不同的名称，但可以是同一张图片） 图片：打开图片时自己命名的变量,如 12#img即图片img=cv2.imread(\"ao_bing.jpg\") 保存：cv2.imwrite(“文件名”,要保存的图像) 总结：完整代码如下： 123456789101112131415161718192021222324#-*- coding:utf-8 -*-#也可以 import cv2 as cv ,使用时用cv代替cv2import cv2#宏定义文件名，便于修改filename=\"ao_bing.jpg\" #这里取的是相对路径#读入图片img=cv2.imread(filename) #默认打开，彩色图像img_gray=cv2.imread(filename,0)#灰度图打开，黑白图像#显示图片cv2.imshow(\"Img\",img)cv2.imshow(\"Img_gray\",img_gray)#使图片长时间停留，不闪退cv2.waitKey(0)#保存图片cv2.imwrite(\"ao_bing_gray.jpg\",img_gray)#摧毁所有窗口cv2.destroyAllWindows()","categories":[{"name":"数字图像处理","slug":"数字图像处理","permalink":"http://uscair.club/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://uscair.club/tags/OpenCV/"}],"author":{"name":"Adans"}},{"title":"Opencv——Python","slug":"Opencv的一些基操作——Python","date":"2019-08-17T09:23:13.000Z","updated":"2019-08-17T09:23:13.000Z","comments":true,"path":"2019/08/17/Opencv的一些基操作——Python/","link":"","permalink":"http://uscair.club/2019/08/17/Opencv%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%93%8D%E4%BD%9C%E2%80%94%E2%80%94Python/","excerpt":"","text":"Opencv——Python首先说一下Opencv是什么：Opencv是一个用图像处理、分析、机器视觉方面的开源数据库，是免费的。 我现在是刚开始学习opencv，而我现在所学习的语言是python语言，在遇到许多问题的时候百度，发现很多案例基本都是用c++写的，而我自己所学的c++没有学很好，所以造成许多需要慢慢探索、慢慢解决的问题。现在我把自己遇到的问题列出来供以后参考。 1.python的程序识别不了中文：通常python程序开头都会写注释 #--coding:utf-8 -- ，但是你可能运行的时候会发现识别不了中文，输出的中文会变成乱码，这是要将这个注释改成 #--coding:cp936 -- 这样就行了，而用opencv调取图片时，如果图片文件名是中文，也有可能识别不了，改成这样同样可以解决问题。2.图片调取代码： 123456789#-*-coding:cp936 -*-import cv2img&#x3D;cv2.imread(&quot;F:\\\\易烊千玺 .jpg&quot;)cv2.namedWindow(&quot;Image&quot;)cv2.imshow(&quot;Image&quot;,img)cv2.waitKey(0)cv2.destroyAllWindows() 运行结果：2.灰度化处理图片。代码： 12gray&#x3D;cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;gray_image&quot;,gray) 承接上一个代码，这样处理图片就会变为灰度图像3.对图像进行灰度变换。先把像素变成一个二维数组，再用函数进行处理代码： 12345678#对灰度图像进行y&#x3D;-x+255g&#x3D;copy.deepcopy(gray)rows&#x3D;img.shape[0]cols&#x3D;img.shape[1]for i in range(rows): for j in range(cols): g[i][j]&#x3D;255-g[i][j]cv2.imshow(&quot;g_image&quot;,g) 4.三色道直方图。代码： 12345678color&#x3D;(&#39;b&#39;,&#39;g&#39;,&#39;r&#39;)def image_hist(image): for i,col in enumerate(color): hist&#x3D;cv2.calcHist([image],[i],None,[256],[0,256]) plt.plot(hist,color&#x3D;col) plt.title(&quot;RGB&quot;) plt.xlim([0,256]) plt.show() 运行结果：这就是三色道直方图。5.灰度直方图。代码： 12plt.hist(gray.ravel(),255,[0,256])plt.show() 6.opencv中的createTrackbar函数。代码： 123456789101112131415161718192021#-*-coding:utf-8 -*-#使用opencv中的creatTrackbar函数来调节一些参数观察图像变化import cv2import numpy as np# 添加新窗口#读入原始图像cv2.namedWindow(&#39;image&#39;)filename &#x3D; &#39;F:\\\\jackson8.jpg&#39;img&#x3D;cv2.imread(filename)def turn(c): num&#x3D;cv2.getTrackbarPos(&quot;num&quot;,&quot;image&quot;) ret,thresh&#x3D; cv2.threshold(img,num,255,cv2.THRESH_BINARY) cv2.imshow(&#39;image&#39;,thresh)cv2.createTrackbar(&#39;num&#39;,&#39;image&#39;, 0,255,turn) # 创建滑块turn(0)cv2.waitKey(0)cv2.destroyAllWindows()","categories":[{"name":"数字图像处理","slug":"数字图像处理","permalink":"http://uscair.club/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"OpenCV","slug":"OpenCV","permalink":"http://uscair.club/tags/OpenCV/"}],"author":{"name":"You_shuangshuang"}},{"title":"卷积神经网络（1）","slug":"卷积神经网络（1）","date":"2019-02-08T16:00:00.000Z","updated":"2019-02-08T16:00:00.000Z","comments":true,"path":"2019/02/09/卷积神经网络（1）/","link":"","permalink":"http://uscair.club/2019/02/09/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%881%EF%BC%89/","excerpt":"","text":"一.概念局部感受野（Local Receptive Fields）卷积神经网络则是把每一个隐藏节点只连接到图像的某个局部区域，从而减少参数训练的数量。例如，一张1024×720的图像，使用9×9的感受野，则只需要81个权值参数。对于一般的视觉也是如此，当观看一张图像时，更多的时候关注的是局部。 共享权值(Shared Weights）卷积神经网络卷积层中，神经元对应的权值是相同，共享的权值和偏置也被称作卷积核或滤汲器。 池化（Pooling)通过对图像卷积进行压缩，卷积之后，通过一个下采样过程，来调整图像大小。Lp池化 监督学习根据已有的数据集，知道输入和输出结果之间的关系。根据这种已知的关系，训练得到一个最优的模型。也就是说，在监督学习中训练数据既有特征(feature)又有标签(label)，通过训练，让机器可以自己找到特征和标签之间的联系，在面对只有特征没有标签的数据时，可以判断出标签。 非监督学习我们不知道数据集中数据、特征之间的关系，而是要根据聚类或一定的模型得到数据之间的关系。 二.结构输入层一维卷积神经网络的输入层接收一维或二维数组，其中一维数组通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组 隐含层由卷积层、池化层和全连接层组成 卷积层~内部包含多个卷积核，功能是对输入数据进行特征提取.上图中间则为卷积核，以下为卷积公式。 池化层在卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。池化层选取池化区域与卷积核扫描特征图步骤相同，由池化大小、步长和填充控制 。 全连接层~ 卷积神经网络中的全连接层等价于传统前馈神经网络中的隐含层。全连接层位于卷积神经网络隐含层的最后部分，并只向其它全连接层传递信号。特征图在全连接层中会失去空间拓扑结构，被展开为向量并通过激励函数 。按表征学习观点，卷积神经网络中的卷积层和池化层能够对输入数据进行特征提取，全连接层的作用则是对提取的特征进行非线性组合以得到输出，即全连接层本身不被期望具有特征提取能力，而是试图利用现有的高阶特征完成学习目标。在一些卷积神经网络中，全连接层的功能可由全局均值池化（global average pooling）取代 ，全局均值池化会将特征图每个通道的所有值取平均，即若有7×7×256的特征图，全局均值池化将返回一个256的向量，其中每个元素都是7×7，步长为7，无填充的均值池化 。 输出层卷积神经网络中输出层的上游通常是全连接层，因此其结构和工作原理与传统前馈神经网络中的输出层相同。对于图像分类问题，输出层使用逻辑函数或归一化指数函数输出分类标签。在物体识别问题中，输出层可设计为输出物体的中心坐标、大小和分类 。在图像语义分割中，输出层直接输出每个像素的分类结果。","categories":[{"name":"数字图像处理","slug":"数字图像处理","permalink":"http://uscair.club/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://uscair.club/tags/CNN/"}],"author":{"name":"ffffff"}}]}