<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>几种常见的卷积神经网络架构 | AIR</title>
  
  <meta name="keywords" content="AI ,NAO ,Scientific Research">
  
  
  <meta name="description" content="AIR">
  

  
  <link rel="alternate" href="/atom.xml" title="AIR">
  

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  

  <!-- link -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.10.1/css/all.min.css">
  

  

  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.11.26/css/style.css">

  

  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
  
  <script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/e92a8350.js","daovoice")</script>

	<meta charset="UTF-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
	<meta http-equiv="X-UA-Compatible" content="ie=edge" />
	<title>Document</title>
	<style type="text/css">
		canvas {
		  position: fixed;
		  right: 0px;
		  bottom: 0px;
		  min-width: 100%;
		  min-height: 100%;
		  height: auto;
		  width: auto;
		  z-index: -1;
		}
	</style>
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="AIR" type="application/atom+xml">
</head>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<body>
  
  
  <div class="cover-wrapper">
    <cover class='cover post half'>
      
        
  <h1 class='title'>AIR</h1>


  <div class="m_search">
    <form name="searchform" class="form u-search-form">
      <input type="text" class="input u-search-input" placeholder="搜索" />
      <i class="icon fas fa-search fa-fw"></i>
    </form>
  </div>

<div class='menu navgation'>
  <ul class='h-list'>
    
      
        <li>
          <a class="nav home" href="/"
            
            
            id="home">
            <i class='fas fa-rss fa-fw'></i>&nbsp;博文
          </a>
        </li>
      
        <li>
          <a class="nav home" href="https://air-usc.github.io/jianli/"
            
              rel="nofollow"
            
            
            id="https:air-usc.github.iojianli">
            <i class='fas fa-code-branch fa-fw'></i>&nbsp;简历
          </a>
        </li>
      
        <li>
          <a class="nav home" href="/about/"
            
              rel="nofollow"
            
            
            id="about">
            <i class='fas fa-info-circle fa-fw'></i>&nbsp;关于
          </a>
        </li>
      
    
  </ul>
</div>

      
    </cover>
    <header class="l_header pure">
  <div id="loading-bar-wrapper">
    <div id="loading-bar" class="pure"></div>
  </div>

	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          AIR
        
      </a>
			<div class='menu navgation'>
				<ul class='h-list'>
          
  					
  						<li>
								<a class="nav flat-box" href="/"
                  
                  
                  id="home">
									<i class='fas fa-grin fa-fw'></i>&nbsp;示例
								</a>
							</li>
      			
  						<li>
								<a class="nav flat-box" href="/archives/"
                  
                    rel="nofollow"
                  
                  
                  id="archives">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="Search" />
						<i class="icon fas fa-search fa-fw"></i>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
        
          <li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
        
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu navgation">
      <ul>
        
          
            <li>
							<a class="nav flat-box" href="/"
                
                
                id="home">
								<i class='fas fa-clock fa-fw'></i>&nbsp;回到主页
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/archives/"
                
                  rel="nofollow"
                
                
                id="archives">
								<i class='fas fa-archive fa-fw'></i>&nbsp;文章归档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="https://xaoxuu.com/wiki/material-x/"
                
                  rel="nofollow"
                
                
                id="https:xaoxuu.comwikimaterial-x">
								<i class='fas fa-book fa-fw'></i>&nbsp;主题文档
							</a>
            </li>
          
            <li>
							<a class="nav flat-box" href="/about/"
                
                  rel="nofollow"
                
                
                id="about">
								<i class='fas fa-info-circle fa-fw'></i>&nbsp;关于小站
							</a>
            </li>
          
       
      </ul>
		</nav>
    </header>
	</aside>
<script>setLoadingBarProgress(40);</script>

  </div>


  <div class="l_body">
    <div class='body-wrapper'>
      <div class='l_main'>
  

  <article id="post" class="post white-box article-type-post" itemscope itemprop="blogPost">
    


  <section class='meta'>
    
      
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.css">
  <div class="aplayer"
    data-theme=""
    data-mini=true 
    
    data-mode="circulation"
    data-server="netease"
    data-type="playlist"
    data-id="2615636388"
    data-volume="0.7">
  </div>
  <script src="https://cdn.jsdelivr.net/npm/aplayer@1.7.0/dist/APlayer.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/meting@1.1.0/dist/Meting.min.js"></script>


    
    
    <div class="meta" id="header-meta">
      
        
  
    <h1 class="title">
      <a href="/2020/05/24/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/">
        几种常见的卷积神经网络架构
      </a>
    </h1>
  


      
      <div class='new-meta-box'>
        
          
        
          
            
  <div class='new-meta-item author'>
    
      <a href="/" rel="nofollow">
        
          <i class="fas fa-user" aria-hidden="true"></i>
        
        <p>Gowi</p>
      </a>
    
  </div>


          
        
          
            <div class="new-meta-item date">
  <a class='notlink'>
    <i class="fas fa-calendar-alt" aria-hidden="true"></i>
    <p>2020-05-24</p>
  </a>
</div>

          
        
          
            
  
  <div class='new-meta-item category'>
    <a href='/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/' rel="nofollow">
      <i class="fas fa-folder-open" aria-hidden="true"></i>
      <p>神经网络</p>
    </a>
  </div>


          
        
          
            
  
    <div class="new-meta-item browse busuanzi">
      <a class='notlink'>
        <i class="fas fa-eye" aria-hidden="true"></i>
        <p>
          <span id="busuanzi_value_page_pv">
            <i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i>
          </span>
        </p>
      </a>
    </div>
  


          
        
          
            
  
    <div class="new-meta-item wordcount">
      <a class='notlink'>
        <i class="fas fa-keyboard" aria-hidden="true"></i>
        <p>字数统计:</p>
        <p>8.1k字</p>
      </a>
    </div>
    <div class="new-meta-item readtime">
      <a class='notlink'>
        <i class="fas fa-hourglass-half" aria-hidden="true"></i>
        <p>阅读时长≈</p>
        <p>37分</p>
      </a>
    </div>
  

          
        
          
            

          
        
      </div>
      
        <hr>
      
    </div>
  </section>


    <section class="article typo">
      <div class="article-entry" itemprop="articleBody">
        <h1 id="几种常见的卷积神经网络架构"><a href="#几种常见的卷积神经网络架构" class="headerlink" title="几种常见的卷积神经网络架构"></a>几种常见的卷积神经网络架构</h1><h2 id="卷积神经网络的基本组成"><a href="#卷积神经网络的基本组成" class="headerlink" title="卷积神经网络的基本组成"></a>卷积神经网络的基本组成</h2><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200517072014.png" alt=""></p>
<h2 id="部分常见的神经网络结构"><a href="#部分常见的神经网络结构" class="headerlink" title="部分常见的神经网络结构"></a>部分常见的神经网络结构</h2><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200517071839.png" alt=""></p>
<h3 id="走向深度：VGGNet"><a href="#走向深度：VGGNet" class="headerlink" title="走向深度：VGGNet"></a>走向深度：VGGNet</h3><blockquote>
<p>2014年的ImageNet亚军，探索了网络深度与性能的关系，用更小的卷积核与更深的网络结构，取得了较好的效果，成为卷积结构发展史上较为重要的一个网络。</p>
</blockquote>
<p>VGGNet采用了五组卷积与三个全连接层，最后使用Softmax做分类。VGGNet有一个显著的特点：每次经过池化层（maxpool）后特征图的尺寸减小一倍，而通道数则增加一 倍（最后一个池化层除外）。</p>
<p>AlexNet中有使用到5×5的卷积核，而在VGGNet中，使用的卷积核 基本都是3×3，而且很多地方出现了多个3×3堆叠的现象，这种结构的优 点在于，首先从感受野来看，两个3×3的卷积核与一个5×5的卷积核是一 样的；其次，同等感受野时，3×3卷积核的参数量更少。更为重要的 是，两个3×3卷积核的非线性能力要比5×5卷积核强，因为其拥有两个激 活函数，可大大提高卷积网络的学习能力。</p>
<h4 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h4><p>在感受野上两个3x3的卷积核等价于一个5x5的卷积核，使用了两个3x3的卷积核来代替5x5的卷积核，减少了参数，同时也增加了非线性能力，提高了学习能力</p>
<p>dropout通过随机删减神经元来减少神经网络的过拟合，在训练时，每个神经元以概率p保留，即以1-p的概率停止工作，每次前向传播保留下来的神经元都不 同，这样可以使得模型不太依赖于某些局部特征，泛化性能更强。在测 试时，为了保证相同的输出期望值，每个参数还要乘以p。</p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200520095042.png" alt=""></p>
<h4 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200520002809.png" alt=""></p>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p>VGG-16</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(VGG, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        in_dim = <span class="number">3</span></span><br><span class="line">        out_dim = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造13个卷积层</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">13</span>):</span><br><span class="line">            layers += [</span><br><span class="line">                nn.Conv2d(in_dim, out_dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>)  <span class="comment"># 可以实现inplace操作，即可以直接将运算结果覆盖到输入中，以节省内存</span></span><br><span class="line">            ]</span><br><span class="line">            in_dim = out_dim</span><br><span class="line">            <span class="comment"># 在第2、4、7、10、13个卷积层后增加池化层</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == <span class="number">3</span> <span class="keyword">or</span> i == <span class="number">6</span> <span class="keyword">or</span> i == <span class="number">9</span> <span class="keyword">or</span> i == <span class="number">12</span>:</span><br><span class="line">                layers += [</span><br><span class="line">                    nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">                ]</span><br><span class="line">                <span class="comment"># 第10个卷积层后保持与第9层第通道数一致，都为512，其余加倍</span></span><br><span class="line">                <span class="keyword">if</span> i != <span class="number">9</span>:</span><br><span class="line">                    out_dim *= <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(*layers)</span><br><span class="line">        <span class="comment"># VGGNet的3个全连接层，中间添加ReLU与Dropout</span></span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        <span class="comment"># 特征图的维度从[1,512,7,7]变到[1,512*7*7]</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    vgg = VGG(<span class="number">21</span>)</span><br><span class="line">    input = torch.randn([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">    print(<span class="string">'input.shape:'</span>, input.shape)</span><br><span class="line">    scores = vgg(input)</span><br><span class="line">    print(<span class="string">'score.shape:'</span>, scores.shape)</span><br><span class="line">    features = vgg.features(input)</span><br><span class="line">    print(<span class="string">'features.shape:'</span>, features.shape)</span><br><span class="line">    print(<span class="string">'vgg.features:'</span>, features.shape)</span><br><span class="line">    print(<span class="string">'vgg.classifier:'</span>, vgg.classifier)</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">input.shape: torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">score.shape: torch.Size([<span class="number">1</span>, <span class="number">21</span>])</span><br><span class="line">features.shape: torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">vgg.features: torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">vgg.classifier: Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=True)</span><br><span class="line">  (<span class="number">1</span>): ReLU(inplace=True)</span><br><span class="line">  (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=False)</span><br><span class="line">  (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=True)</span><br><span class="line">  (<span class="number">4</span>): ReLU(inplace=True)</span><br><span class="line">  (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=False)</span><br><span class="line">  (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">21</span>, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="从横交错：Inception（GoogLeNet）"><a href="#从横交错：Inception（GoogLeNet）" class="headerlink" title="从横交错：Inception（GoogLeNet）"></a>从横交错：Inception（GoogLeNet）</h3><blockquote>
<p>出自Google听说其命名是为了致敬LeNet，神经网络向广度发展的一个典型</p>
</blockquote>
<p>Inception v1网络一共有9个上述堆叠的模块，共有22层，在最后的 Inception模块处使用了全局平均池化。为了避免深层网络训练时带来的 梯度消失问题，作者还引入了两个辅助的分类器，在第3个与第6个 Inception模块输出后执行Softmax并计算损失，在训练时和最后的损失一 并回传。其中1×1的模块可以先将特征图 降维，再送给3×3和5×5大小的卷积核，由于通道数的降低，参数量也有了较大的减少。</p>
<p>Inception v2增加了 BN层，同时利用两个级联的3×3卷积取代了Inception v1版本中的5×5卷 积，这种方式既减少了卷积参数量，也增加了网络的非<br>线性能力。</p>
<h4 id="改进-1"><a href="#改进-1" class="headerlink" title="改进"></a>改进</h4><p>使用BN层</p>
<p>BN层首先对每一个batch的输入特征进行白化操作，即去均值方差 过程。假设一个batch的输入数据为x：$B=｛x_1,…,x_m｝$，首先求该batch 数据的均值与方差<br>$$<br>u_B\gets\frac{1}{m}\sum_{i=1}^{m}x_i<br>$$</p>
<p>$$<br>\sigma_B^2\gets\frac{1}{m}\sum_{i=1}^{m}(x_i-u_B)^2<br>$$</p>
<p>以上公式中，m代表batch的大小，$μ_B$为批处理数据的均值，$σ^2_B$为 批处理数据的方差。在求得均值方差后，利用下面公式进行去均值方差操作：<br>$$<br>\widehat{x}_i\gets\frac{x_i-u_i}{\sqrt{\sigma^2+\varepsilon}}<br>$$<br>白化操作可以使输入的特征分布具有相同的均值与方差，固定了每 一层的输入分布，从而加速网络的收敛。然而，白化操作虽然从一定程 度上避免了梯度饱和，但也限制了网络中数据的表达能力，浅层学到的 参数信息会被白化操作屏蔽掉，因此，BN层在白化操作后又增加了一 个线性变换操作，让数据尽可能地恢复本身的表达能力<br>$$<br>y_i\gets \gamma\widehat{x}_i+\beta<br>$$<br>γ与β为新引进的可学习参数，最终的输出为yi。 BN层可以看做是增加了线性变换的白化操作，在实际工程中被证 明了能够缓解神经网络难以训练的问题。BN层的优点主要有以下3点： </p>
<ul>
<li>缓解梯度消失，加速网络收敛。BN层可以让激活函数的输入数据 落在非饱和区，缓解了梯度消失问题。此外，由于每一层数据的均值与 方差都在一定范围内，深层网络不必去不断适应浅层网络输入的变化， 实现了层间解耦，允许每一层独立学习，也加快了网络的收敛。 </li>
<li>简化调参，网络更稳定。在调参时，学习率调得过大容易出现震 荡与不收敛，BN层则抑制了参数微小变化随网络加深而被放大的问 题，因此对于参数变化的适应能力更强，更容易调参。 </li>
<li>防止过拟合。BN层将每一个batch的均值与方差引入到网络中，由 于每个batch的这两个值都不相同，可看做为训练过程增加了随机噪音，可以起到一定的正则效果，防止过拟合。</li>
</ul>
<p>在测试时，由于是对单个样本进行测试，没有batch的均值与方差， 通常做法是在训练时将每一个batch的均值与方差都保留下来，在测试时 使用所有训练样本均值与方差的平均值。</p>
<h4 id="神经网络结构-1"><a href="#神经网络结构-1" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p>inceptionV1</p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200520093430.png" alt=""></p>
<p>inceptionV2</p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200520093616.png" alt=""></p>
<h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><p>Inception v1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个conv+relu的类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicConv2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, kernel_size, padding=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义googLeNet</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inceptionv1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, hid_1_1, hid_2_1, hid_2_3, hid_3_1, out_3_5, out_4_1)</span>:</span></span><br><span class="line">        super(Inceptionv1, self).__init__()</span><br><span class="line">        <span class="comment"># 定义四个子模块</span></span><br><span class="line">        self.branch1x1 = BasicConv2d(in_dim, hid_1_1, <span class="number">1</span>)</span><br><span class="line">        self.branch3x3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_dim, hid_2_1, <span class="number">1</span>),</span><br><span class="line">            BasicConv2d(hid_2_1, hid_2_3, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch5x5 = nn.Sequential(</span><br><span class="line">            BasicConv2d(in_dim, hid_3_1, <span class="number">1</span>),</span><br><span class="line">            BasicConv2d(hid_3_1, out_3_5, <span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch_pool = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            BasicConv2d(in_dim, out_4_1, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        b1 = self.branch1x1(x)</span><br><span class="line">        b2 = self.branch3x3(x)</span><br><span class="line">        b3 = self.branch5x5(x)</span><br><span class="line">        b4 = self.branch_pool(x)</span><br><span class="line">        <span class="comment"># 连接子模块</span></span><br><span class="line">        output = torch.cat((b1, b2, b3, b4), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    net_inceptionv1 = Inceptionv1(<span class="number">3</span>, <span class="number">63</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>, <span class="number">96</span>, <span class="number">32</span>)</span><br><span class="line">    print(<span class="string">'net_inceptionv1:'</span>, net_inceptionv1)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">    print(input.shape)</span><br><span class="line">    output = net_inceptionv1(input)</span><br><span class="line">    print(<span class="string">'output.shape'</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">net_inceptionv1: Inceptionv1(</span><br><span class="line">  (branch1x1): BasicConv2d(</span><br><span class="line">    (conv): Conv2d(<span class="number">3</span>, <span class="number">63</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  )</span><br><span class="line">  (branch3x3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (branch5x5): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">64</span>, <span class="number">96</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (branch_pool): Sequential(</span><br><span class="line">    (<span class="number">0</span>): MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=False)</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>])</span><br><span class="line">output.shape torch.Size([<span class="number">1</span>, <span class="number">255</span>, <span class="number">256</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure>

<p>Inception v2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicConv2d</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, kernel_size, padding=<span class="number">0</span>)</span>:</span></span><br><span class="line">        super(BasicConv2d, self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_channels, eps=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inceptionv2</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Inceptionv2, self).__init__()</span><br><span class="line">        self.branch1 = BasicConv2d(<span class="number">192</span>, <span class="number">96</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.branch2 = nn.Sequential(</span><br><span class="line">            BasicConv2d(<span class="number">192</span>, <span class="number">48</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">            BasicConv2d(<span class="number">48</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch3 = nn.Sequential(</span><br><span class="line">            BasicConv2d(<span class="number">192</span>, <span class="number">64</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">            BasicConv2d(<span class="number">64</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">            BasicConv2d(<span class="number">96</span>, <span class="number">96</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.branch4 = nn.Sequential(</span><br><span class="line">            nn.AvgPool2d(<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, count_include_pad=<span class="literal">False</span>),</span><br><span class="line">            BasicConv2d(<span class="number">192</span>, <span class="number">64</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x0 = self.branch1(x)</span><br><span class="line">        x1 = self.branch2(x)</span><br><span class="line">        x2 = self.branch3(x)</span><br><span class="line">        x3 = self.branch4(x)</span><br><span class="line">        out = torch.cat((x0, x1, x2, x3), <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    net_inceptionv2 = Inceptionv2()</span><br><span class="line">    print(<span class="string">'net_inceptionv2:'</span>, net_inceptionv2)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">    print(<span class="string">'input.shape:'</span>, input.shape)</span><br><span class="line">    output = net_inceptionv2(input)</span><br><span class="line">    print(<span class="string">'output.shape:'</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">net_inceptionv2: Inceptionv2(</span><br><span class="line">  (branch1): BasicConv2d(</span><br><span class="line">    (conv): Conv2d(<span class="number">192</span>, <span class="number">96</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (bn): BatchNorm2d(<span class="number">96</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">  )</span><br><span class="line">  (branch2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">192</span>, <span class="number">48</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">48</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">48</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (branch3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">192</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">64</span>, <span class="number">96</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">96</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">96</span>, <span class="number">96</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">96</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (branch4): Sequential(</span><br><span class="line">    (<span class="number">0</span>): AvgPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">    (<span class="number">1</span>): BasicConv2d(</span><br><span class="line">      (conv): Conv2d(<span class="number">192</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (bn): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">0.001</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">input.shape: torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">output.shape: torch.Size([<span class="number">1</span>, <span class="number">320</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br></pre></td></tr></table></figure>

<h3 id="里程碑：ResNet"><a href="#里程碑：ResNet" class="headerlink" title="里程碑：ResNet"></a>里程碑：ResNet</h3><blockquote>
<p>何凯明大神的作品,较好的解决了梯度下降的问题，获得了2015年ImageNet分类任务的第一名</p>
</blockquote>
<p>ResNet（Residual Network，残差网络）较好的解决了梯度下降的问题，获得了2015年ImageNet分类任务的第一名。此后的分类、检测、分割等 任务也大规模使用ResNet作为网络骨架。 </p>
<h4 id="改进-2"><a href="#改进-2" class="headerlink" title="改进"></a>改进</h4><p>ResNet的思想在于引入了一个深度残差框架来解决梯度消失问题， 即让卷积网络去学习残差映射，而不是期望每一个堆叠层的网络都完整 地拟合潜在的映射（拟合函数）。如图所示，对于神经网络，如果 我们期望的网络最终映射为H(x)，左侧的网络需要直接拟合输出H(x)， 而右侧由ResNet提出的子模块，通过引入一个shortcut（捷径）分支，将 需要拟合的映射变为残差F(x)：H(x)-x。ResNet给出的假设是：相较于 直接优化潜在映射H(x)，优化残差映射F(x)是更为容易的。由于F(x)+x是逐通道进行相加，因此根据两者是否通道数相同，存 在两种Bottleneck结构。对于通道数不同的情况，比如每个卷积组的第 一个Bottleneck，需要利用1×1卷积对x进行Downsample操作，将通道数变为相同，再进行加操作。对于相同的情况下，两者可以直接进行相 加。</p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521084337.png" alt=""></p>
<h4 id="神经网络结构-2"><a href="#神经网络结构-2" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521085504.png" alt=""></p>
<h4 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, out_dim, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        <span class="comment"># 网络堆叠层是由1x1、3x3、1x1这3个卷积层组成，中间包含BN层</span></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, in_dim, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(in_dim),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_dim, in_dim, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(in_dim),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_dim, out_dim, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_dim)</span><br><span class="line">        )</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># Downsample部分由一个包含BN层的1x1的卷积组成</span></span><br><span class="line">        self.downsample = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_dim, out_dim, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        identity = self.downsample(x)</span><br><span class="line">        <span class="comment"># 将identity（恒等映射）与网络堆叠层进行相加，并经过ReLU输出</span></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    bottleneck_1_1 = Bottleneck(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line">    print(<span class="string">'bottleneck_1_1'</span>, bottleneck_1_1)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>)</span><br><span class="line">    output = bottleneck_1_1(input)</span><br><span class="line">    print(<span class="string">'input.shape'</span>, input.shape)</span><br><span class="line">    print(<span class="string">'output.shape'</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">bottleneck_1_1 <span class="title">Bottleneck</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">  (bottleneck): Sequential(</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">2</span>): ReLU(inplace=True)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">5</span>): ReLU(inplace=True)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span></span></span><br><span class="line"><span class="function"><span class="params">  )</span></span></span><br><span class="line"><span class="function"><span class="params">  (relu): ReLU(inplace=True)</span></span></span><br><span class="line"><span class="function"><span class="params">  (downsample): Sequential(</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span></span></span><br><span class="line"><span class="function"><span class="params">    (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span></span></span><br><span class="line"><span class="function"><span class="params">  )</span></span></span><br><span class="line"><span class="function"><span class="params">)</span></span></span><br><span class="line">input.shape torch.Size([1, 64, 56, 56])</span><br><span class="line">output.shape torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br></pre></td></tr></table></figure>

<h3 id="继往开来：DenseNet"><a href="#继往开来：DenseNet" class="headerlink" title="继往开来：DenseNet"></a>继往开来：DenseNet</h3><blockquote>
<p>2017CVPR的best论文奖</p>
</blockquote>
<p>ResNet通过前层与后层的“短路连接”（Shortcuts），加强 了前后层之间的信息流通，在一定程度上缓解了梯度消失现象，从而可 以将神经网络搭建得很深。DenseNet最大化了这 种前后层信息交流，通过建立前面所有层与后面层的密集连接，实现了 特征在通道维度上的复用，使其可以在参数与计算量更少的情况下实现 比ResNet更优的性能。</p>
<h4 id="改进-3"><a href="#改进-3" class="headerlink" title="改进"></a>改进</h4><p>FPN将深层的语义信息传到底层，来补充浅层的语义信息，从而获 得了高分辨率、强语义的特征，在小物体检测、实例分割等领域有着非 常不俗的表现。</p>
<p>相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L层的网络，DenseNet共包含$\frac{L(L+1)}{2}$  个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。</p>
<h4 id="神经网络结构-3"><a href="#神经网络结构-3" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521133258.png" alt=""></p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521133348.png" alt=""></p>
<p>网络由多个Dense Block与中间 的卷积池化组成，核心就在Dense Block中。Dense Block中的黑点代表 一个卷积层，其中的多条黑线代表数据的流动，每一层的输入由前面的 所有卷积层的输出组成。注意这里使用了通道拼接（Concatnate）操 作，而非ResNet的逐元素相加操作。</p>
<p>DenseNet的结构有如下两个特性： </p>
<ul>
<li>神经网络一般需要使用池化等操作缩小特征图尺寸来提取语义特 征，而Dense Block需要保持每一个Block内的特征图尺寸一致来直接进 行Concatnate操作，因此DenseNet被分成了多个Block。Block的数量一 般为4。</li>
<li>两个相邻的Dense Block之间的部分被称为Transition层，具体包括 BN、ReLU、1×1卷积、2×2平均池化操作。1×1卷积的作用是降维，起 到压缩模型的作用，而平均池化则是降低特征图的尺寸， 具体的Block实现细节如图3.20所示，每一个Block由若干个 Bottleneck的卷积层组成，对应图3.19中的黑点。Bottleneck由BN、 ReLU、1×1卷积、BN、ReLU、3×3卷积的顺序构成。</li>
</ul>
<p>关于Block，有以下4个细节需要注意： </p>
<ul>
<li>每一个Bottleneck输出的特征通道数是相同的，例如这里的32。同 时可以看到，经过Concatnate操作后的通道数是按32的增长量增加的， 因此这个32也被称为GrowthRate。 </li>
<li>这里1×1卷积的作用是固定输出通道数，达到降维的作用。当几十 个Bottleneck相连接时，Concatnate后的通道数会增加到上千，如果不增 加1×1的卷积来降维，后续3×3卷积所需的参数量会急剧增加。1×1卷积 的通道数通常是GrowthRate的4倍。 </li>
<li>特征传递方式是直接将前面所有层的特征Concatnate后 传到下一层，这种方式与具体代码实现的方式是一致的，而不像图3.19 中，前面层都要有一个箭头指向后面的所有层。</li>
<li>Block采用了激活函数在前、卷积层在后的顺序，这与一般的网络 上是不同的。</li>
</ul>
<h4 id="代码实现-3"><a href="#代码实现-3" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nChannels, growthRate)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        interChannels = <span class="number">4</span> * growthRate</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(nChannels)</span><br><span class="line">        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(interChannels)</span><br><span class="line">        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(F.relu(self.bn1(x)))</span><br><span class="line">        out = self.conv2(F.relu(self.bn2(out)))</span><br><span class="line">        out = torch.cat((x, out), <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Denseblock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nChannels, growthRate, nDenseBlock)</span>:</span></span><br><span class="line">        super(Denseblock, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(int(nDenseBlock)):</span><br><span class="line">            layers.append(Bottleneck(nChannels, growthRate))</span><br><span class="line">            nChannels += growthRate</span><br><span class="line">        self.denseblock = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.denseblock(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    denseblock = Denseblock(<span class="number">64</span>, <span class="number">32</span>, <span class="number">6</span>)</span><br><span class="line">    print(<span class="string">'denseblock:'</span>, denseblock)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">    output = denseblock(input)</span><br><span class="line">    print(<span class="string">'output.shape:'</span>, output.shape)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">denseblock: Denseblock(</span><br><span class="line">  (denseblock): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">1</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">96</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">96</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">2</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">3</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">160</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">160</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">4</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">192</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">192</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">    (<span class="number">5</span>): Bottleneck(</span><br><span class="line">      (bn1): BatchNorm2d(<span class="number">224</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv1): Conv2d(<span class="number">224</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (bn2): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (conv2): Conv2d(<span class="number">128</span>, <span class="number">32</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">output.shape: torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure>

<h3 id="特征金字塔：FPN"><a href="#特征金字塔：FPN" class="headerlink" title="特征金字塔：FPN"></a>特征金字塔：FPN</h3><blockquote>
<p>为了解决多尺度问题</p>
</blockquote>
<p>为了增强语义性，传统的物体检测模型通常只在深度卷积网络的最 后一个特征图上进行后续操作，而这一层对应的下采样率（图像缩小的 倍数）通常又比较大，如16、32，造成小物体在特征图上的有效信息较 少，小物体的检测性能会急剧下降，这个问题也被称为多尺度问题。</p>
<p>解决多尺度问题的关键在于如何提取多尺度的特征。传统的方法有 图像金字塔（Image Pyramid），主要思路是将输入图片做成多个尺度， 不同尺度的图像生成不同尺度的特征，这种方法简单而有效，大量使用 在了COCO等竞赛上，但缺点是非常耗时，计算量也很大。</p>
<h4 id="改进-4"><a href="#改进-4" class="headerlink" title="改进"></a>改进</h4><p>2017年的FPN（Feature Pyramid Network）方法融合了不同层的特征，较好地改善了多尺度检测问题。</p>
<h4 id="神经网络结构-4"><a href="#神经网络结构-4" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521143814.png" alt=""></p>
<ul>
<li>自下而上：最左侧为普通的卷积网络，默认使用ResNet结构，用 作提取语义信息。C1代表了ResNet的前几个卷积与池化层，而C2至C5 分别为不同的ResNet卷积组，这些卷积组包含了多个Bottleneck结构， 组内的特征图大小相同，组间大小递减。 </li>
<li>自上而下：首先对C5进行1×1卷积降低通道数得到P5，然后依次进 行上采样得到P4、P3和P2，目的是得到与C4、C3与C2长宽相同的特 征，以方便下一步进行逐元素相加。这里采用2倍最邻近上采样，即直 接对临近元素进行复制，而非线性插值。 </li>
<li>横向连接（Lateral Connection）：目的是为了将上采样后的高语义 特征与浅层的定位细节特征进行融合。高语义特征经过上采样后，其长 宽与对应的浅层特征相同，而通道数固定为256，因此需要对底层特征 C2至C4进行1x1卷积使得其通道数变为256，然后两者进行逐元素相加得 到P4、P3与P2。由于C1的特征图尺寸较大且语义信息不足，因此没有 把C1放到横向连接中。</li>
<li>卷积融合：在得到相加后的特征后，利用3×3卷积对生成的P2至P4 再进行融合，目的是消除上采样过程带来的重叠效应，以生成最终的特 征图。</li>
</ul>
<h4 id="代码实现-4"><a href="#代码实现-4" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span>  <span class="comment"># 通道倍增数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_place, planes, stride=<span class="number">1</span>, downsample=None)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_place, planes, <span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(planes),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(planes, planes, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(planes),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(planes, self.expansion * planes, <span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(self.expansion * planes)</span><br><span class="line">        )</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        identity = x</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># layers代表每一个阶段的Bottleneck的数量</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FPN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers)</span>:</span></span><br><span class="line">        super(FPN, self).__init__()</span><br><span class="line">        self.inplanes = <span class="number">64</span></span><br><span class="line">        <span class="comment"># 处理输入的C1模块</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>, <span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 搭建自上而下的C2、C3、C4、C5模块</span></span><br><span class="line">        self.layer1 = self._make_layer(<span class="number">64</span>, layers[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(<span class="number">128</span>, layers[<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(<span class="number">256</span>, layers[<span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(<span class="number">512</span>, layers[<span class="number">3</span>], <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 对C5减少通道数，得到P5</span></span><br><span class="line">        self.toplayer = nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 3x3卷积特征融合</span></span><br><span class="line">        self.smooth1 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.smooth2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.smooth3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 横向连接，保证通道数相同</span></span><br><span class="line">        self.latlayer1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.latlayer2 = nn.Conv2d(<span class="number">512</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        self.latlayer3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建C2到C5，注意区分stride=1或2的情况</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, planes, blocks, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.inplanes != Bottleneck.expansion * planes:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.inplanes, Bottleneck.expansion * planes, <span class="number">1</span>, stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(Bottleneck.expansion * planes)</span><br><span class="line">            )</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(Bottleneck(self.inplanes, planes, stride, downsample))</span><br><span class="line">        self.inplanes = planes * Bottleneck.expansion</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, blocks):</span><br><span class="line">            layers.append(Bottleneck(self.inplanes, planes))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自上而下的上采样模块</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_upsample_add</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        _, _, H, W = y.shape</span><br><span class="line">        <span class="keyword">return</span> F.upsample(x, size=(H, W), mode=<span class="string">'bilinear'</span>) + y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 自下而上</span></span><br><span class="line">        c1 = self.maxpool(self.relu(self.bn1(self.conv1(x))))</span><br><span class="line">        c2 = self.layer1(c1)</span><br><span class="line">        c3 = self.layer2(c2)</span><br><span class="line">        c4 = self.layer3(c3)</span><br><span class="line">        c5 = self.layer4(c4)</span><br><span class="line">        <span class="comment"># 自上而下</span></span><br><span class="line">        p5 = self.toplayer(c5)</span><br><span class="line">        p4 = self._upsample_add(p5, self.latlayer1(c4))</span><br><span class="line">        p3 = self._upsample_add(p4, self.latlayer2(c3))</span><br><span class="line">        p2 = self._upsample_add(p3, self.latlayer3(c2))</span><br><span class="line">        <span class="comment"># 卷积融合，平滑处理</span></span><br><span class="line">        p4 = self.smooth1(p4)</span><br><span class="line">        p3 = self.smooth2(p3)</span><br><span class="line">        p2 = self.smooth3(p2)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> p2, p3, p4, p5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    net_fpn = FPN([<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line">    print(<span class="string">'net_fpn.conv1:'</span>, net_fpn.conv1)</span><br><span class="line">    print(<span class="string">'net_fpn.bn1:'</span>, net_fpn.bn1)</span><br><span class="line">    print(<span class="string">'net_fpn.relu:'</span>, net_fpn.relu)</span><br><span class="line">    print(<span class="string">'net_fpn.maxpool:'</span>, net_fpn.maxpool)</span><br><span class="line">    print(<span class="string">'net_fpn.layer1:'</span>, net_fpn.layer1)</span><br><span class="line">    print(<span class="string">'net_fpn.layer2:'</span>, net_fpn.layer2)</span><br><span class="line">    print(<span class="string">'net_fpn.toplayer:'</span>, net_fpn.toplayer)</span><br><span class="line">    print(<span class="string">'net_fpn.smooth1:'</span>, net_fpn.smooth1)</span><br><span class="line">    print(<span class="string">'net_fpn.latlayer1'</span>, net_fpn.latlayer1)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">    output = net_fpn(input)</span><br><span class="line">    print(<span class="string">'output[0].shape:'</span>, output[<span class="number">0</span>].shape)</span><br><span class="line">    print(<span class="string">'output[1].shape:'</span>, output[<span class="number">1</span>].shape)</span><br><span class="line">    print(<span class="string">'output[2].shape:'</span>, output[<span class="number">2</span>].shape)</span><br><span class="line">    print(<span class="string">'output[3].shape:'</span>, output[<span class="number">3</span>].shape)</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">net_fpn.conv1: Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">3</span>, <span class="number">3</span>), bias=False)</span><br><span class="line">net_fpn.bn1: BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">net_fpn.relu: ReLU(inplace=True)</span><br><span class="line">net_fpn.maxpool: MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>, dilation=<span class="number">1</span>, ceil_mode=False)</span><br><span class="line">net_fpn.layer1: Sequential(</span><br><span class="line">  (<span class="number">0</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">    (downsample): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">1</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">2</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">64</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">64</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">64</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">256</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">net_fpn.layer2: Sequential(</span><br><span class="line">  (<span class="number">0</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">    (downsample): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">2</span>), bias=False)</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">1</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">2</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">3</span>): Bottleneck(</span><br><span class="line">    (bottleneck): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">1</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">2</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">3</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), bias=False)</span><br><span class="line">      (<span class="number">4</span>): BatchNorm2d(<span class="number">128</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">      (<span class="number">5</span>): ReLU(inplace=True)</span><br><span class="line">      (<span class="number">6</span>): Conv2d(<span class="number">128</span>, <span class="number">512</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">      (<span class="number">7</span>): BatchNorm2d(<span class="number">512</span>, eps=<span class="number">1e-05</span>, momentum=<span class="number">0.1</span>, affine=True, track_running_stats=True)</span><br><span class="line">    )</span><br><span class="line">    (relu): ReLU(inplace=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">net_fpn.toplayer: Conv2d(<span class="number">2048</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">net_fpn.smooth1: Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">net_fpn.<span class="function">latlayer1 <span class="title">Conv2d</span><span class="params">(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span></span></span><br><span class="line">output[0].shape: torch.Size([1, 256, 56, 56])</span><br><span class="line">output[<span class="number">1</span>].shape: torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">output[<span class="number">2</span>].shape: torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">output[<span class="number">3</span>].shape: torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br></pre></td></tr></table></figure>

<h3 id="为检测而生：DetNet"><a href="#为检测而生：DetNet" class="headerlink" title="为检测而生：DetNet"></a>为检测而生：DetNet</h3><p>如VGGNet和ResNet等，虽从各个角度出发 提升了物体检测性能，但究其根本是为ImageNet的图像分类任务而设计 的。而图像分类与物体检测两个任务天然存在着落差，分类任务侧重于 全图的特征提取，深层的特征图分辨率很低；而物体检测需要定位出物 体位置，特征图分辨率不宜过小，因此造成了以下两种缺陷： </p>
<ul>
<li><p>大物体难以定位：对于FPN等网络，大物体对应在较深的特征图上 检测，由于网络较深时下采样率较大，物体的边缘难以精确预测，增加 了回归边界的难度。 </p>
</li>
<li><p>小物体难以检测：对于传统网络，由于下采样率大造成小物体在 较深的特征图上几乎不可见；FPN虽从较浅的特征图来检测小物体，但 浅层的语义信息较弱，且融合深层特征时使用的上采样操作也会增加物 体检测的难度。</p>
<p>针对以上问题，旷视科技提出了专为物体检测设计的DetNet结构， 引入了空洞卷积，使得模型兼具较大感受野与较高分辨率，同时避免了FPN的多次上采样，实现了较好的检测效果。</p>
</li>
</ul>
<p>DetNet的网络结构如图3.22所示，仍然选择性能优越的ResNet-50作 为基础结构，并保持前4个stage与ResNet-50相同，具体的结构细节有以 下3点：</p>
<ul>
<li>引入了一个新的Stage 6，用于物体检测。Stage 5与Stage 6使用了 DetNet提出的Bottleneck结构，最大的特点是利用空洞数为2的3×3卷积 取代了步长为2的3×3卷积。</li>
<li>Stage 5与Stage 6的每一个Bottleneck输出的特征图尺寸都为原图的$\frac{1}{16}$ ，通道数都为256，而传统的Backbone通常是特征图尺寸递减，通道数递增。</li>
<li>在组成特征金字塔时，由于特征图大小完全相同，因此可以直接 从右向左传递相加，避免了上一节的上采样操作。为了进一步融合各通 道的特征，需要对每一个阶段的输出进行1×1卷积后再与后一Stage传回 的特征相加。</li>
</ul>
<h4 id="改进-5"><a href="#改进-5" class="headerlink" title="改进"></a>改进</h4><p>采用了空洞卷积</p>
<p>图a是普通的卷积过程，在卷积核紧密排列在特征图上滑动计算，而图b代表了空洞数为2的空洞 卷积，可以看到，在特征图上每2行或者2列选取元素与卷积核卷积。类 似地，图c代表了空洞数为3的空洞卷积。</p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521223627.png" alt=""></p>
<p>在代码实现时，空洞卷积有一个额外的超参数dilation rate，表示空<br>洞数，普通卷积dilation rate默认为1，图中的b与c的dilation rate分别 为2与3。</p>
<p>在图3.11中，同样的一个3×3卷积，却可以起到5×5、7×7等卷积的 效果。可以看出，空洞卷积在不增加参数量的前提下，增大了感受野。 假设空洞卷积的卷积核大小为k，空洞数为d，则其等效卷积核大小k’计 算如式所示<br>$$<br>k’=k+(k-1)\times (d-1)<br>$$<br>空洞卷积的优点显而易见，在不引入额外参数的前提下可以任意扩 大感受野，同时保持特征图的分辨率不变。这一点在分割与检测任务中 十分有用，感受野的扩大可以检测大物体，而特征图分辨率不变使得物 体定位更加精准。</p>
<p>当然，空洞卷积也有自己的一些缺陷，主要表现在以下3个方面：</p>
<ul>
<li>网格效应（Gridding Effect）：由于空洞卷积是一种稀疏的采样方<br>  式，当多个空洞卷积叠加时，有些像素根本没有被利用到，会损失信息 的连续性与相关性，进而影响分割、检测等要求较高的任务。 </li>
<li>远距离的信息没有相关性：空洞卷积采取了稀疏的采样方式，导 致远距离卷积得到的结果之间缺乏相关性，进而影响分类的结果。 </li>
<li>不同尺度物体的关系：大的dilation rate对于大物体分割与检测有 利，但是对于小物体则有弊无利，如何处理好多尺度问题的检测，是空 洞卷积设计的重点。</li>
</ul>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/v2-d552433faa8363df84c53b905443a556_hd.gif" alt=""></p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/v2-4959201e816888c6648f2e78cccfd253_hd.gif" alt=""></p>
<h4 id="神经网络结构-5"><a href="#神经网络结构-5" class="headerlink" title="神经网络结构"></a>神经网络结构</h4><p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521230605.png" alt=""></p>
<p><img src="https://gowi-picgo.oss-cn-shenzhen.aliyuncs.com/picgo/20200521230538.png" alt=""></p>
<h4 id="代码实现-5"><a href="#代码实现-5" class="headerlink" title="代码实现"></a>代码实现</h4><p>Bottlenck A与Bottlenck B</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DetBottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, planes, stride=<span class="number">1</span>, extra=False)</span>:</span></span><br><span class="line">        super(DetBottleneck, self).__init__()</span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inplanes, planes, <span class="number">1</span>, bias=<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(planes),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>, dilation=<span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(planes),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(planes, planes, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(planes)</span><br><span class="line">        )</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.extra = extra</span><br><span class="line">        <span class="keyword">if</span> self.extra:</span><br><span class="line">            self.extra_conv = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inplanes, planes, <span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(planes)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.extra:</span><br><span class="line">            identity = self.extra_conv(x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            identity = x</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    bottleneck_b = DetBottleneck(<span class="number">1024</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="literal">True</span>)</span><br><span class="line">    print(<span class="string">'bottleneck_b:'</span>, bottleneck_b)</span><br><span class="line">    bottleneck_a1 = DetBottleneck(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">    print(<span class="string">'bottleneck_a1:'</span>, bottleneck_a1)</span><br><span class="line">    bottleneck_a2 = DetBottleneck(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">    print(<span class="string">'bottleneck_a2:'</span>, bottleneck_a2)</span><br><span class="line">    input = torch.randn(<span class="number">1</span>, <span class="number">1024</span>, <span class="number">14</span>, <span class="number">14</span>)</span><br><span class="line">    output1 = bottleneck_b(input)</span><br><span class="line">    output2 = bottleneck_a1(output1)</span><br><span class="line">    output3 = bottleneck_a2(output2)</span><br><span class="line">    print(<span class="string">'output1.shape:'</span>, output1.shape)</span><br><span class="line">    print(<span class="string">'output2.shape:'</span>, output2.shape)</span><br><span class="line">    print(<span class="string">'output3.shape:'</span>, output3.shape)</span><br></pre></td></tr></table></figure>
      </div>
	  
            <! -- 添加版权信息 -->
<div class="article-footer-copyright">
<center>本文由<b><a href="/index.html" target="_blank" title="USC-AIR">USC-AIR</a></b>创作和发表,采用<b>BY</b>-<b>NC</b>-<b>SA</b>国际许可协议进行许可</center>
 
<center>转载请注明作者及出处,本文作者为<b><a href="/index.html" target="_blank" title="USC-AIR">USC-AIR</a></b>,本文标题为<b><a href="/2020/05/24/几种常见的卷积神经网络架构/" target="_blank" title="几种常见的卷积神经网络架构">几种常见的卷积神经网络架构</a></b></center>
 
<center>本文链接为<b><a href="/2020/05/24/几种常见的卷积神经网络架构/" target="_blank" title="几种常见的卷积神经网络架构">http://uscair.club/2020/05/24/几种常见的卷积神经网络架构/</a></b>.</center>
</div>
<! -- 添加版权信息 -->
        
      
      
        <br>
        


  <section class='meta' id="footer-meta">
    <div class='new-meta-box'>
      
        
          <div class="new-meta-item date" itemprop="dateUpdated" datetime="2020-05-24T00:00:00+08:00">
  <a class='notlink'>
    <i class="fas fa-clock" aria-hidden="true"></i>
    <p>updated at May 24, 2020</p>
  </a>
</div>

        
      
        
          
  
  <div class="new-meta-item meta-tags"><a class="tag" href="/tags/CNN/" rel="nofollow"><i class="fas fa-tag" aria-hidden="true"></i><p>CNN</p></a></div>


        
      
        
          
  <div class="new-meta-item share -mob-share-list">
  <div class="-mob-share-list share-body">
    
      
        <a class="-mob-share-qq" title="QQ好友" rel="external nofollow noopener noreferrer"
          
          href="http://connect.qq.com/widget/shareqq/index.html?url=http://uscair.club/2020/05/24/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/&title=几种常见的卷积神经网络架构 | AIR&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qq.png">
          
        </a>
      
    
      
        <a class="-mob-share-qzone" title="QQ空间" rel="external nofollow noopener noreferrer"
          
          href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=http://uscair.club/2020/05/24/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/&title=几种常见的卷积神经网络架构 | AIR&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/qzone.png">
          
        </a>
      
    
      
        <a class="-mob-share-weibo" title="微博" rel="external nofollow noopener noreferrer"
          
          href="http://service.weibo.com/share/share.php?url=http://uscair.club/2020/05/24/%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84/&title=几种常见的卷积神经网络架构 | AIR&summary="
          
          >
          
            <img src="https://cdn.jsdelivr.net/gh/xaoxuu/assets@19.1.9/logo/128/weibo.png">
          
        </a>
      
    
  </div>
</div>



        
      
    </div>
  </section>


      
      
          <div class="prev-next">
              
              
                  <section class="next">
                      <span class="art-item-right" aria-hidden="true">
                          <h6>Next&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i></h6>
                          <h4>
                              <a href="/2020/05/17/%E4%B8%8D%E4%BE%9D%E9%9D%A0%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%EF%BC%88%E9%99%A4%E4%BA%86numpy%EF%BC%89%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="prev" title="使神经网络具有记忆力——RNN及LSTM">
                                  
                                      使神经网络具有记忆力——RNN及LSTM
                                  
                              </a>
                          </h4>
                          
                              
                              <h6 class="tags">
                                  <a class="tag" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><i class="fas fa-tag fa-fw" aria-hidden="true"></i> 神经网络</a>
                              </h6>
                          
                      </span>
                  </section>
              
          </div>
      
    </section>
  </article>



  <!-- 显示推荐文章和评论 -->



  <article class="post white-box comments">
    <section class="article typo">
      <h4><i class="fas fa-comments fa-fw" aria-hidden="true"></i>&nbsp;Comments</h4>
      
      
        <section id="comments">
          <div id="lv-container" data-id="city" data-uid="MTAyMC80ODI4NC8yNDc3OA==">
            <noscript><div><i class='fas fa-exclamation-triangle'>&nbsp;Unable to load Livere, please make sure your network can access.</div></noscript>
          </div>
        </section>
      
      
      
    </section>
  </article>






<!-- 根据页面mathjax变量决定是否加载MathJax数学公式js -->

  <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": {
      preferredFont: "TeX",
      availableFonts: ["STIX","TeX"],
      linebreaks: { automatic:true },
      EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
      inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
      processEscapes: true,
      ignoreClass: "tex2jax_ignore|dno",
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: { autoNumber: "AMS" },
      noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
      Macros: { href: "{}" }
    },
    messageStyle: "none"
  });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += (all[i].SourceElement().parentNode.className ? ' ' : '') + 'has-jax';
    }
  });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>




  <script>
    window.subData = {
      title: '几种常见的卷积神经网络架构',
      tools: true
    }
  </script>


</div>
<aside class='l_side'>
  
    
    
      
      
        
          
          
            
              <section class='widget author'>
  <div class='content pure'>
    
	<a href="https://github.com/AIRUSC" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
      <div class='avatar'>
        <img class='avatar' src='https://img.vim-cn.com/b5/c85ca5730c5889d433b1d0958d0d1d0b478687.png'/>
      </div>
    
    
      <div class='text'>
        
        
        
          <p><span id="jinrishici-sentence">AIR</span></p>
          <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
        
      </div>
    
    
      <div class="social-wrapper">
        
          
            <a href="/atom.xml"
              class="social fas fa-rss flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="mailto:airusc@foxmail.com"
              class="social fas fa-envelope flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
          
            <a href="https://github.com/AIRUSC"
              class="social fab fa-github flat-btn"
              target="_blank"
              rel="external nofollow noopener noreferrer">
            </a>
          
        
      </div>
    
	<hr/>
	<font color="#2EF0FF" >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;访客地图 </font>
  </div>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=597ckzulxto&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
</section>

            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
            
              
  <section class='widget toc-wrapper'>
    
<header class='pure'>
  <div><i class="fas fa-list fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;TOC</div>
  
    <!-- <div class='wrapper'><a class="s-toc rightBtn" rel="external nofollow noopener noreferrer" href="javascript:void(0)"><i class="fas fa-thumbtack fa-fw"></i></a></div> -->
  
</header>

    <div class='content pure'>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#几种常见的卷积神经网络架构"><span class="toc-text">几种常见的卷积神经网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积神经网络的基本组成"><span class="toc-text">卷积神经网络的基本组成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#部分常见的神经网络结构"><span class="toc-text">部分常见的神经网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#走向深度：VGGNet"><span class="toc-text">走向深度：VGGNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#从横交错：Inception（GoogLeNet）"><span class="toc-text">从横交错：Inception（GoogLeNet）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进-1"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构-1"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现-1"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#里程碑：ResNet"><span class="toc-text">里程碑：ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进-2"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构-2"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现-2"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#继往开来：DenseNet"><span class="toc-text">继往开来：DenseNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进-3"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构-3"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现-3"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征金字塔：FPN"><span class="toc-text">特征金字塔：FPN</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进-4"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构-4"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现-4"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#为检测而生：DetNet"><span class="toc-text">为检测而生：DetNet</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#改进-5"><span class="toc-text">改进</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#神经网络结构-5"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#代码实现-5"><span class="toc-text">代码实现</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
  </section>


            
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
            
              <section class='widget grid'>
  
<header class='pure'>
  <div><i class="fas fa-map-signs fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;站内导航</div>
  
</header>

  <div class='content pure'>
    <ul class="grid navgation">
      
        <li><a class="flat-box" title="/" href="/"
          
          
          id="home">
          
            <i class="fas fa-clock fa-fw" aria-hidden="true"></i>
          
          近期文章
        </a></li>
      
        <li><a class="flat-box" title="/archives/" href="/archives/"
          
            rel="nofollow"
          
          
          id="archives">
          
            <i class="fas fa-archive fa-fw" aria-hidden="true"></i>
          
          文章归档
        </a></li>
      
        <li><a class="flat-box" title="https://xaoxuu.com/wiki/material-x/" href="https://xaoxuu.com/wiki/material-x/"
          
            rel="nofollow"
          
          
          id="https:xaoxuu.comwikimaterial-x">
          
            <i class="fas fa-book fa-fw" aria-hidden="true"></i>
          
          主题文档
        </a></li>
      
        <li><a class="flat-box" title="/about/" href="/about/"
          
            rel="nofollow"
          
          
          id="about">
          
            <i class="fas fa-info-circle fa-fw" aria-hidden="true"></i>
          
          关于小站
        </a></li>
      
    </ul>
  </div>
</section>

            
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
            
              
  <section class='widget category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;Categories</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/blog/categories/"
    title="blog/categories/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/C/" href="/categories/C/"><div class='name'>C++</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Hexo/" href="/categories/Hexo/"><div class='name'>Hexo</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/NAO/" href="/categories/NAO/"><div class='name'>NAO</div><div class='badge'>(5)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Naoqi/" href="/categories/Naoqi/"><div class='name'>Naoqi</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Python-web/" href="/categories/Python-web/"><div class='name'>Python web</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Python/" href="/categories/Python/"><div class='name'>Python</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box" title="/categories/Python%E7%BC%96%E7%A8%8B/" href="/categories/Python%E7%BC%96%E7%A8%8B/"><div class='name'>Python编程</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/openCV/" href="/categories/openCV/"><div class='name'>openCV</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box child" title="/categories/openCV/%E5%9B%BE%E7%89%87%E5%8F%98%E5%8C%96/" href="/categories/openCV/%E5%9B%BE%E7%89%87%E5%8F%98%E5%8C%96/"><div class='name'>图片变化</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box" title="/categories/python/" href="/categories/python/"><div class='name'>python</div><div class='badge'>(7)</div></a></li>
        
          <li><a class="flat-box" title="/categories/python%E5%9F%BA%E7%A1%80/" href="/categories/python%E5%9F%BA%E7%A1%80/"><div class='name'>python基础</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/rcnn/" href="/categories/rcnn/"><div class='name'>rcnn</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/test/" href="/categories/test/"><div class='name'>test</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"><div class='name'>人工智能</div><div class='badge'>(6)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E5%8D%95%E7%89%87%E6%9C%BA/" href="/categories/%E5%8D%95%E7%89%87%E6%9C%BA/"><div class='name'>单片机</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" href="/categories/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"><div class='name'>数字图像处理</div><div class='badge'>(14)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/" href="/categories/%E6%99%BA%E8%83%BD%E7%AE%97%E6%B3%95/"><div class='name'>智能算法</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><div class='name'>机器学习</div><div class='badge'>(33)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%93%E6%A0%8F/" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%93%E6%A0%8F/"><div class='name'>机器学习专栏</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/"><div class='name'>机器学习基础</div><div class='badge'>(7)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><div class='name'>深度学习</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E6%A0%8F/" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%93%E6%A0%8F/"><div class='name'>深度学习专栏</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><div class='name'>神经网络</div><div class='badge'>(4)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E7%AE%97%E6%B3%95/" href="/categories/%E7%AE%97%E6%B3%95/"><div class='name'>算法</div><div class='badge'>(2)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86/" href="/categories/%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86/"><div class='name'>网络原理</div><div class='badge'>(3)</div></a></li>
        
          <li><a class="flat-box" title="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"><div class='name'>计算机视觉</div><div class='badge'>(1)</div></a></li>
        
      </ul>
    </div>
  </section>


            
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
            
              
  <section class='widget tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;Hot Tags</div>
  
    <a class="rightBtn"
    
      rel="nofollow"
    
    
    href="/blog/tags/"
    title="blog/tags/">
    <i class="fas fa-expand-arrows-alt fa-fw"></i></a>
  
</header>

    <div class='content pure'>
      <a href="/tags/AI%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 18.29px; color: #7c7c7c">AI大数据</a> <a href="/tags/CNN/" style="font-size: 21.14px; color: #686868">CNN</a> <a href="/tags/DNN/" style="font-size: 14px; color: #999">DNN</a> <a href="/tags/Django/" style="font-size: 14px; color: #999">Django</a> <a href="/tags/GUI/" style="font-size: 14px; color: #999">GUI</a> <a href="/tags/LSTM/" style="font-size: 14px; color: #999">LSTM</a> <a href="/tags/NAO/" style="font-size: 14px; color: #999">NAO</a> <a href="/tags/NAOqi/" style="font-size: 16.86px; color: #868686">NAOqi</a> <a href="/tags/Naoqi%E6%9C%BA%E5%99%A8%E4%BA%BA/" style="font-size: 14px; color: #999">Naoqi机器人</a> <a href="/tags/Numpy/" style="font-size: 14px; color: #999">Numpy</a> <a href="/tags/OpenCV/" style="font-size: 22.57px; color: #5f5f5f">OpenCV</a> <a href="/tags/Python/" style="font-size: 19.71px; color: #727272">Python</a> <a href="/tags/Pytorh/" style="font-size: 15.43px; color: #8f8f8f">Pytorh</a> <a href="/tags/RNN/" style="font-size: 14px; color: #999">RNN</a> <a href="/tags/STM32/" style="font-size: 14px; color: #999">STM32</a> <a href="/tags/STM32F407/" style="font-size: 15.43px; color: #8f8f8f">STM32F407</a> <a href="/tags/TensorFlow/" style="font-size: 15.43px; color: #8f8f8f">TensorFlow</a> <a href="/tags/python/" style="font-size: 16.86px; color: #868686">python</a> <a href="/tags/rcnn-%E6%A0%87%E7%AD%BE/" style="font-size: 15.43px; color: #8f8f8f">rcnn#标签</a> <a href="/tags/robocup/" style="font-size: 14px; color: #999">robocup</a> <a href="/tags/test/" style="font-size: 14px; color: #999">test</a> <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15.43px; color: #8f8f8f">人工智能</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF/" style="font-size: 14px; color: #999">图像增广</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 15.43px; color: #8f8f8f">图像处理</a> <a href="/tags/%E5%9B%BE%E7%89%87%E5%8F%98%E5%8C%96/" style="font-size: 14px; color: #999">图片变化</a> <a href="/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/" style="font-size: 14px; color: #999">多线程</a> <a href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/" style="font-size: 14px; color: #999">字符串</a> <a href="/tags/%E5%B0%8F%E7%99%BD%E5%AD%A6%E4%B9%A0/" style="font-size: 14px; color: #999">小白学习</a> <a href="/tags/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF/" style="font-size: 14px; color: #999">拉普拉斯</a> <a href="/tags/%E6%8E%A7%E5%88%B6/" style="font-size: 14px; color: #999">控制</a> <a href="/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/" style="font-size: 14px; color: #999">数字图像处理</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86/" style="font-size: 15.43px; color: #8f8f8f">数据集预处理</a> <a href="/tags/%E6%95%B0%E7%90%86%E5%9F%BA%E7%A1%80/" style="font-size: 14px; color: #999">数理基础</a> <a href="/tags/%E6%96%87%E4%BB%B6/" style="font-size: 14px; color: #999">文件</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 22.57px; color: #5f5f5f">机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" style="font-size: 24px; color: #555">机器学习基础</a> <a href="/tags/%E6%A8%A1%E6%9D%BF/" style="font-size: 14px; color: #999">模板</a> <a href="/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" style="font-size: 14px; color: #999">监督学习</a> <a href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" style="font-size: 14px; color: #999">目标检测</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 16.86px; color: #868686">神经网络</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 21.14px; color: #686868">笔记</a> <a href="/tags/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0/" style="font-size: 14px; color: #999">算法学习</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 14px; color: #999">线性回归</a> <a href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6/" style="font-size: 14px; color: #999">统计学</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86/" style="font-size: 16.86px; color: #868686">网络原理</a> <a href="/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" style="font-size: 16.86px; color: #868686">聚类算法</a>
    </div>
  </section>


            
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
          
          
        
      
    

  
</aside>

<footer id="footer" class="clearfix">
  
  
    <div class="social-wrapper">
      
        
          <a href="/atom.xml"
            class="social fas fa-rss flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="mailto:airusc@foxmail.com"
            class="social fas fa-envelope flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
        
          <a href="https://github.com/AIRUSC"
            class="social fab fa-github flat-btn"
            target="_blank"
            rel="external nofollow noopener noreferrer">
          </a>
        
      
    </div>
  
  <br>
  <div><p>Blog content follows the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener">Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) License</a></p>
</div>
  <div>
    Use
    <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a>
    as theme
    
      , 
      total visits
      <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span>
      times
    
    . 
  </div>
  
  <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("01/13/2020 16:36:00");//在此处修改你的建站时间，格式：月/日/年 时:分:秒
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
  
</footer>
<script>setLoadingBarProgress(80);</script>



      <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
  </div>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>

<script src="//instant.page/1.2.2" type="module" integrity="sha384-2xV8M5griQmzyiY3CDqh1dn4z3llDVqZDqzjzcY+jCBCk/a5fXJmuZ/40JJAPeoU"></script>


  <script async src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
  <script type="text/javascript">
    $(function() {
      const $reveal = $('.reveal');
      if ($reveal.length === 0) return;
      const sr = ScrollReveal({ distance: 0 });
      sr.reveal('.reveal');
    });
  </script>


  <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
  <script type="text/javascript">
    $(function() {
      Waves.attach('.flat-btn', ['waves-button']);
      Waves.attach('.float-btn', ['waves-button', 'waves-float']);
      Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
      Waves.attach('.flat-box', ['waves-block']);
      Waves.attach('.float-box', ['waves-block', 'waves-float']);
      Waves.attach('.waves-image');
      Waves.init();
    });
  </script>


  <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>




  
  
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-backstretch/2.0.4/jquery.backstretch.min.js"></script>
    <script type="text/javascript">
      $(function(){
        if ('.cover') {
          $('.cover').backstretch(
          ["http://gss0.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/00e93901213fb80e3db9657d31d12f2eb938942b.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        } else {
          $.backstretch(
          ["http://gss0.baidu.com/-fo3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/00e93901213fb80e3db9657d31d12f2eb938942b.jpg"],
          {
            duration: "6000",
            fade: "2500"
          });
        }
      });
    </script>
  






  <script type="text/javascript">
    (function(d, s) {
      var j, e = d.getElementsByTagName(s)[0];
      if (typeof LivereTower === 'function') { return; }
      j = d.createElement(s);
      j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
      j.async = true;
      e.parentNode.insertBefore(j, e);
    })(document, 'script');
  </script>






  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.11/js/app.js"></script>



  
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@19.11/js/search.js"></script>





<!-- 复制 -->
<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  let COPY_SUCCESS = "Copied";
  let COPY_FAILURE = "Copy failed";
  /*页面载入完成后，创建复制按钮*/
  !function (e, t, a) {
    /* code */
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '  <i class="fa fa-copy"></i><span>Copy</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });

      clipboard.on('success', function(e) {
        //您可以加入成功提示
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        success_prompt(COPY_SUCCESS);
        e.clearSelection();
      });
      clipboard.on('error', function(e) {
        //您可以加入失败提示
        console.error('Action:', e.action);
        console.error('Trigger:', e.trigger);
        fail_prompt(COPY_FAILURE);
      });
    }
    initCopyCode();

  }(window, document);

  /**
   * 弹出式提示框，默认1.5秒自动消失
   * @param message 提示信息
   * @param style 提示样式，有alert-success、alert-danger、alert-warning、alert-info
   * @param time 消失时间
   */
  var prompt = function (message, style, time)
  {
      style = (style === undefined) ? 'alert-success' : style;
      time = (time === undefined) ? 1500 : time*1000;
      $('<div>')
          .appendTo('body')
          .addClass('alert ' + style)
          .html(message)
          .show()
          .delay(time)
          .fadeOut();
  };

  // 成功提示
  var success_prompt = function(message, time)
  {
      prompt(message, 'alert-success', time);
  };

  // 失败提示
  var fail_prompt = function(message, time)
  {
      prompt(message, 'alert-danger', time);
  };

  // 提醒
  var warning_prompt = function(message, time)
  {
      prompt(message, 'alert-warning', time);
  };

  // 信息提示
  var info_prompt = function(message, time)
  {
      prompt(message, 'alert-info', time);
  };

</script>


<!-- fancybox -->
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>
<script>
  let LAZY_LOAD_IMAGE = "";
  $(".article-entry").find("fancybox").find("img").each(function () {
      var element = document.createElement("a");
      $(element).attr("data-fancybox", "gallery");
      $(element).attr("href", $(this).attr("src"));
      /* 图片采用懒加载处理时,
       * 一般图片标签内会有个属性名来存放图片的真实地址，比如 data-original,
       * 那么此处将原本的属性名src替换为对应属性名data-original,
       * 修改如下
       */
       if (LAZY_LOAD_IMAGE) {
         $(element).attr("href", $(this).attr("data-original"));
       }
      $(this).wrap(element);
  });
</script>





  <script>setLoadingBarProgress(100);</script>
  
  <canvas id="canvas" width="1440" height="900" ></canvas>
<script>
	window.onload = function(){
    //获取画布对象
    var canvas = document.getElementById("canvas");
    //获取画布的上下文
    var context =canvas.getContext("2d");
    var s = window.screen;
    var W = canvas.width = s.width;
    var H = canvas.height;
    //获取浏览器屏幕的宽度和高度
    //var W = window.innerWidth;
    //var H = window.innerHeight;
    //设置canvas的宽度和高度
    canvas.width = W;
    canvas.height = H;
    //每个文字的字体大小
    var fontSize = 12;
    //计算列
    var colunms = Math.floor(W /fontSize);	
    //记录每列文字的y轴坐标
    var drops = [];
    //给每一个文字初始化一个起始点的位置
    for(var i=0;i<colunms;i++){
        drops.push(0);
    }
    //运动的文字
    var str ="WELCOME TO SH.ITRHX.COM";
    //4:fillText(str,x,y);原理就是去更改y的坐标位置
    //绘画的函数
    function draw(){
        context.fillStyle = "rgba(238,238,238,.08)";//遮盖层
        context.fillRect(0,0,W,H);
        //给字体设置样式
        context.font = "600 "+fontSize+"px  Georgia";
        //给字体添加颜色
        context.fillStyle = ["#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF", "#2EF0FF"][parseInt(Math.random() * 10)];//randColor();可以rgb,hsl, 标准色，十六进制颜色
        //写入画布中
        for(var i=0;i<colunms;i++){
            var index = Math.floor(Math.random() * str.length);
            var x = i*fontSize;
            var y = drops[i] *fontSize;
            context.fillText(str[index],x,y);
            //如果要改变时间，肯定就是改变每次他的起点
            if(y >= canvas.height && Math.random() > 0.99){
                drops[i] = 0;
            }
            drops[i]++;
        }
    };
    function randColor(){//随机颜色
        var r = Math.floor(Math.random() * 256);
        var g = Math.floor(Math.random() * 256);
        var b = Math.floor(Math.random() * 256);
        return "rgb("+r+","+g+","+b+")";
    }
    draw();
    setInterval(draw,35);
};
</script>
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/miku.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
<script type='text/javascript' src='/js/DaoVoice.js'></script>

<!--彩带2.自动飘动-->

    <!-- <script src="https://g.joyinshare.com/hc/piao.js" type="text/javascript"></script> -->
    <script type="text/javascript" src="/js/ribbon_flow.js"></script>


<!--动态线条背景-->
<!--<script type="text/javascript"-->
<!--color="56,239,243" opacity='0.9' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">-->
<!--</script>-->
</html>
